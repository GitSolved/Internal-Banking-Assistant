# Foundation-Sec-8B Configuration for Cybersecurity RAG System
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: true
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_ingestion:
    enabled: ${LOCAL_INGESTION_ENABLED:true}
    allow_ingest_from: ["*"]
  local_data_folder: local_data/internal_assistant

ui:
  enabled: true
  path: /
  default_mode: "General LLM"
  default_chat_system_prompt: >
    Answer questions directly and concisely.
  default_query_system_prompt: >
    Use document context to answer questions.
  default_summarization_system_prompt: >
    Provide a concise summary.
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

llm:
  mode: llamacpp
  prompt_style: "foundation-sec"  # Use custom prompt style for Foundation-Sec-8B
  max_new_tokens: 256     # PrivateGPT default for faster responses
  context_window: 3900    # PrivateGPT default for optimal performance
  temperature: 0.1        # PrivateGPT default for consistent responses

rag:
  similarity_top_k: 4     # PrivateGPT default for faster document retrieval
  similarity_value: 0.2   # PrivateGPT default threshold
  rerank:
    enabled: false        # Disabled for optimal performance speed
    model: cross-encoder/ms-marco-MiniLM-L-2-v2
    top_n: 2              # PrivateGPT default

summarize:
  use_async: true

clickhouse:
    host: localhost
    port: 8443
    username: admin
    password: clickhouse
    database: embeddings

llamacpp:
  # Foundation-Sec-8B Configuration
  llm_hf_repo_id: local/Foundation-Sec-8B
  llm_hf_model_file: Foundation-Sec-8B-q4_k_m.gguf
  # PrivateGPT default parameters for optimal performance
  tfs_z: 1.0              # Tail free sampling disabled
  top_k: 40               # PrivateGPT default
  top_p: 0.9              # PrivateGPT default
  repeat_penalty: 1.1     # PrivateGPT default
  n_threads: 8            # PrivateGPT default - more reasonable for your system
  n_batch: 128            # PrivateGPT default - more reasonable for your system
  n_gpu_layers: 0         # CPU-only mode for Intel Iris Xe graphics
  verbose: false          # Disable verbose logging

embedding:
  mode: huggingface
  ingest_mode: simple
  embed_dim: 768          # For nomic-ai/nomic-embed-text-v1.5

huggingface:
  embedding_hf_model_name: nomic-ai/nomic-embed-text-v1.5
  access_token: ${HF_TOKEN:}
  trust_remote_code: true

vectorstore:
  database: qdrant

nodestore:
  database: simple

milvus:
  uri: local_data/internal_assistant/milvus/milvus_local.db
  collection_name: milvus_db
  overwrite: false

qdrant:
  path: local_data/internal_assistant/qdrant

postgres:
  host: localhost
  port: 5432
  database: postgres
  user: postgres
  password: postgres
  schema_name: internal_assistant

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo
  embedding_api_key: ${OPENAI_API_KEY:}

ollama:
  llm_model: foundation-sec:8b
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434
  keep_alive: 5m
  tfs_z: 1.0
  top_k: 40
  top_p: 0.9
  repeat_last_n: 64
  repeat_penalty: 1.2
  request_timeout: 120.0
  autopull_models: true

azopenai:
  api_key: ${AZ_OPENAI_API_KEY:}
  azure_endpoint: ${AZ_OPENAI_ENDPOINT:}
  embedding_deployment_name: ${AZ_OPENAI_EMBEDDING_DEPLOYMENT_NAME:}
  llm_deployment_name: ${AZ_OPENAI_LLM_DEPLOYMENT_NAME:}
  api_version: "2023-05-15"
  embedding_model: text-embedding-ada-002
  llm_model: gpt-35-turbo

gemini:
  api_key: ${GOOGLE_API_KEY:}
  model: models/gemini-pro
  embedding_model: models/embedding-001
