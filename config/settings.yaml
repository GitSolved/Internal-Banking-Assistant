# The default configuration file for Internal Assistant.
# This configuration is optimized for cybersecurity intelligence workflows.
# Syntax defined in `internal_assistant/settings/settings.py`
#
# IMPORTANT: This configuration uses specific dependency versions to ensure compatibility:
# - FastAPI: >=0.108.0,<0.115.0 (avoids Pydantic schema generation issues)
# - Pydantic: >=2.8.0,<2.9.0 (compatible with LlamaIndex)
# - Gradio: >=4.15.0,<4.39.0 (avoids FastAPI integration issues)
# - Python: 3.11.9 (required for all dependencies)
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: true
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_ingestion:
    enabled: ${LOCAL_INGESTION_ENABLED:true}
    allow_ingest_from: ["*"]
  local_data_folder: local_data/internal_assistant

ui:
  enabled: true
  path: /
  # "RAG Mode" enables RAG with your ingested documents, "General LLM" is direct chat
  default_mode: "RAG Mode"
  default_chat_system_prompt: >
    You are a banking compliance and cybersecurity intelligence assistant.
    Provide accurate, well-cited guidance on regulatory requirements, security threats,
    and compliance obligations. When discussing regulations, cite specific sections
    (e.g., "12 CFR 30", "Reg E", "FFIEC guidance"). For security threats, reference
    MITRE ATT&CK techniques when applicable. Maintain a professional, precise tone
    suitable for banking compliance and IT security professionals.
  default_query_system_prompt: >
    Analyze the provided regulatory documents and compliance materials to answer questions.
    Always cite specific sources, page numbers, and effective dates when available.
    If regulatory guidance conflicts or has been updated, note the chronology and current
    requirements. For compliance deadlines, clearly state implementation dates and any
    phase-in periods. Structure responses to be audit-ready and documentation-friendly.
  default_summarization_system_prompt: >
    Summarize regulatory updates, compliance requirements, or security advisories concisely.
    Highlight: (1) What changed, (2) Effective dates and deadlines, (3) Affected institutions
    or systems, (4) Required actions, (5) Compliance implications. Use bullet points for
    action items. Maintain regulatory terminology and cite source documents.
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

llm:
  mode: ollama  # Switch to Ollama for better performance
  prompt_style: "llama3"  # Use Llama 3 chat template for proper formatting
  # Should be matching the selected model
  max_new_tokens: 2048  # Increased for Llama 3.1 70B - allow full banking compliance responses
  context_window: 4096  # Matches Ollama model config
  # Select your tokenizer. Llama-index tokenizer is the default.
  # tokenizer: local/Foundation-Sec-8B
  temperature: 0.1      # Low temperature for consistent, factual responses

rag:
  similarity_top_k: 4   # Optimal for M3 Max - balanced speed and context richness
  #This value controls how many "top" documents the RAG returns to use in the context.
  similarity_value: 0.15  # Accept moderately relevant chunks (typical scores: 0.15-0.35)
  #This value is disabled by default.  If you enable this settings, the RAG will only use articles that meet a certain percentage score.
  rerank:
    enabled: false        # Disabled for optimal performance speed in CTI workflows
    model: cross-encoder/ms-marco-MiniLM-L-2-v2

summarize:
  use_async: true

clickhouse:
    host: localhost
    port: 8443
    username: admin
    password: clickhouse
    database: embeddings

ollama:
  llm_model: llama31-70b-m3max  # Llama 3.1 70B Instruct - M3 Max optimized with 40-core GPU acceleration
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434  # change if your embedding model runs on another ollama
  keep_alive: 5m
  request_timeout: 180.0  # Increased for maximum token generation (16K tokens)
  num_predict: 16384  # MAXIMUM output tokens (~12,000 words / ~25 pages)
  autopull_models: false  # We already have the model
  # M3 Max specific optimizations (applied via Modelfile, documented here for reference)
  # num_gpu: 40 (all GPU cores)
  # num_thread: 12 (leave 4 CPU cores for system)
  # num_batch: 256 (optimal for 70B model on M3 Max unified memory)

llamacpp:
  llm_hf_repo_id: local/Foundation-Sec-8B
  llm_hf_model_file: Foundation-Sec-8B-q4_k_m.gguf
  tfs_z: 1.0
  top_k: 40
  top_p: 0.9
  repeat_penalty: 1.1
  n_threads: 8
  n_batch: 128
  n_gpu_layers: 0
  verbose: false

embedding:
  # Should be matching the value above in most cases
  mode: huggingface
  ingest_mode: simple
  embed_dim: 768 # 768 is for nomic-ai/nomic-embed-text-v1.5
  # Optimized for cybersecurity content processing with available RAM
  batch_size: 256  # Increased batch size for faster processing
  max_length: 256  # Reduced for faster text processing

huggingface:
  embedding_hf_model_name: nomic-ai/nomic-embed-text-v1.5
  access_token: ${HF_TOKEN:}
  # sentence-transformers models are fully supported by HuggingFace Transformers
  # Local model requires trust_remote_code for custom architecture
  trust_remote_code: true

vectorstore:
  database: qdrant

nodestore:
  database: simple

milvus:
  uri: local_data/internal_assistant/milvus/milvus_local.db
  collection_name: milvus_db
  overwrite: false

qdrant:
  path: local_data/internal_assistant/qdrant
  collection_name: internal_assistant_documents

postgres:
  host: localhost
  port: 5432
  database: postgres
  user: postgres
  password: postgres
  schema_name: internal_assistant

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo
  embedding_api_key: ${OPENAI_API_KEY:}

azopenai:
  api_key: ${AZ_OPENAI_API_KEY:}
  azure_endpoint: ${AZ_OPENAI_ENDPOINT:}
  embedding_deployment_name: ${AZ_OPENAI_EMBEDDING_DEPLOYMENT_NAME:}
  llm_deployment_name: ${AZ_OPENAI_LLM_DEPLOYMENT_NAME:}
  api_version: "2023-05-15"
  embedding_model: text-embedding-ada-002
  llm_model: gpt-35-turbo

gemini:
  api_key: ${GOOGLE_API_KEY:}
  model: models/gemini-pro
  embedding_model: models/embedding-001
