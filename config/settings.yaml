# The default configuration file for Internal Assistant.
# This configuration is optimized for cybersecurity intelligence workflows.
# Syntax defined in `internal_assistant/settings/settings.py`
#
# IMPORTANT: This configuration uses specific dependency versions to ensure compatibility:
# - FastAPI: >=0.108.0,<0.115.0 (avoids Pydantic schema generation issues)
# - Pydantic: >=2.8.0,<2.9.0 (compatible with LlamaIndex)
# - Gradio: >=4.15.0,<4.39.0 (avoids FastAPI integration issues)
# - Python: 3.11.9 (required for all dependencies)
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: true
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_ingestion:
    enabled: ${LOCAL_INGESTION_ENABLED:true}
    allow_ingest_from: ["*"]
  local_data_folder: local_data/internal_assistant

ui:
  enabled: true
  path: /
  # "RAG Mode" or "General LLM"
  default_mode: "General LLM"
  default_chat_system_prompt: >
    Answer questions directly and concisely.
  default_query_system_prompt: >
    Use document context to answer questions.
  default_summarization_system_prompt: >
    Provide a concise summary.
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

# Security Forums & Communities Directory Configuration
forum_directory:
  # Enable or disable forum directory feature
  enabled: true

  # Default category filter ("All", "Professional", "Dark Web", "CTF & Training", "Bug Bounty", "Specialized")
  default_category: "All"

  # Forum access types to display
  access_types:
    clearnet: true      # Show clearnet forums (accessible via standard browsers)
    darkweb: false      # Show dark web forums (require Tor Browser) - DISABLED to prevent onion links

  # Export settings
  export:
    enabled: true
    default_format: "json"  # Options: "json", "csv", "markdown"
    export_directory: "local_data/internal_assistant/exports"
    include_metadata: true  # Include category, access type, and description in exports

  # Display settings
  display:
    max_forums_per_category: 50  # Maximum forums to display per category
    show_category_badges: true    # Show colored category badges
    show_access_indicators: true  # Show clearnet/darkweb indicators
    show_descriptions: true       # Show forum descriptions
    show_statistics: true         # Show forum count statistics in header

  # Category configuration
  categories:
    professional:
      enabled: true
      display_name: "Professional"
      icon: "🌐"
      color: "#0077BE"
      description: "Professional security communities and forums"

    darkweb:
      enabled: true
      display_name: "Dark Web"
      icon: "🔒"
      color: "#6F42C1"
      description: "Dark web forums (Tor required)"

    ctf_training:
      enabled: true
      display_name: "CTF & Training"
      icon: "🎯"
      color: "#5E35B1"
      description: "Capture The Flag and security training platforms"

    bug_bounty:
      enabled: true
      display_name: "Bug Bounty"
      icon: "💰"
      color: "#FF6B35"
      description: "Bug bounty and vulnerability disclosure platforms"

    specialized:
      enabled: true
      display_name: "Specialized"
      icon: "🔧"
      color: "#20C997"
      description: "Specialized security forums and communities"

  # Refresh settings for dark web forum scraping
  refresh:
    enabled: true
    # Auto-refresh interval in minutes (0 to disable auto-refresh)
    interval_minutes: 0  # Disabled by default - refresh manually to avoid rate limiting
    tor_taxi_url: "https://tor.taxi/forums"  # URL for dark web forum directory scraping

  # Security warnings
  security:
    show_warnings: true
    warning_message: >
      Dark web forums require Tor Browser for access.
      Exercise caution and follow all applicable laws.
      This directory is for research and threat intelligence purposes only.

llm:
  mode: ollama  # Switch to Ollama for better performance
  prompt_style: "llama2"
  # Should be matching the selected model
  max_new_tokens: 1000  # Significantly increased for larger responses
  context_window: 8192  # Significantly increased for larger files
  # Select your tokenizer. Llama-index tokenizer is the default.
  # tokenizer: local/Foundation-Sec-8B
  temperature: 0.1      # Low temperature for consistent, factual responses

rag:
  similarity_top_k: 8   # Significantly increased for better document coverage
  #This value controls how many "top" documents the RAG returns to use in the context.
  similarity_value: 0.1  # Much lower threshold to include more relevant documents
  #This value is disabled by default.  If you enable this settings, the RAG will only use articles that meet a certain percentage score.
  rerank:
    enabled: false        # Disabled for optimal performance speed in CTI workflows
    model: cross-encoder/ms-marco-MiniLM-L-2-v2

summarize:
  use_async: true

clickhouse:
    host: localhost
    port: 8443
    username: admin
    password: clickhouse
    database: embeddings

ollama:
  llm_model: foundation-sec-q4km:latest  # Use the new q4_k_m model for better speed
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434  # change if your embedding model runs on another ollama
  keep_alive: 5m
  request_timeout: 60.0  # Increased timeout for larger model
  autopull_models: false  # We already have the model

llamacpp:
  llm_hf_repo_id: local/Foundation-Sec-8B
  llm_hf_model_file: Foundation-Sec-8B-q4_k_m.gguf
  tfs_z: 1.0
  top_k: 40
  top_p: 0.9
  repeat_penalty: 1.1
  n_threads: 8
  n_batch: 128
  n_gpu_layers: 0
  verbose: false

embedding:
  # Should be matching the value above in most cases
  mode: huggingface
  ingest_mode: simple
  embed_dim: 768 # 768 is for nomic-ai/nomic-embed-text-v1.5
  # Optimized for cybersecurity content processing with available RAM
  batch_size: 256  # Increased batch size for faster processing
  max_length: 256  # Reduced for faster text processing

huggingface:
  embedding_hf_model_name: nomic-ai/nomic-embed-text-v1.5
  access_token: ${HF_TOKEN:}
  # sentence-transformers models are fully supported by HuggingFace Transformers
  # Local model requires trust_remote_code for custom architecture
  trust_remote_code: true

vectorstore:
  database: qdrant

nodestore:
  database: simple

milvus:
  uri: local_data/internal_assistant/milvus/milvus_local.db
  collection_name: milvus_db
  overwrite: false

qdrant:
  path: local_data/internal_assistant/qdrant
  collection_name: internal_assistant_documents

postgres:
  host: localhost
  port: 5432
  database: postgres
  user: postgres
  password: postgres
  schema_name: internal_assistant

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo
  embedding_api_key: ${OPENAI_API_KEY:}

azopenai:
  api_key: ${AZ_OPENAI_API_KEY:}
  azure_endpoint: ${AZ_OPENAI_ENDPOINT:}
  embedding_deployment_name: ${AZ_OPENAI_EMBEDDING_DEPLOYMENT_NAME:}
  llm_deployment_name: ${AZ_OPENAI_LLM_DEPLOYMENT_NAME:}
  api_version: "2023-05-15"
  embedding_model: text-embedding-ada-002
  llm_model: gpt-35-turbo

gemini:
  api_key: ${GOOGLE_API_KEY:}
  model: models/gemini-pro
  embedding_model: models/embedding-001
