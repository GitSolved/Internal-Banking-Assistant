# Llama 3.1 70B Instruct - M3 Max Optimized Configuration
# Optimized for Apple M3 Max (40-core GPU, 64GB unified memory)
# Banking compliance and IT security workflows

FROM llama3.1:70b-instruct-q4_K_M

# Llama 3.1 chat template with proper formatting
TEMPLATE """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|><|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{{ .Response }}<|eot_id|>"""

# System prompt for banking compliance and IT security
SYSTEM """You are a banking compliance and IT security expert assistant. You provide accurate, well-researched answers about:

- Banking regulations (FDIC, SEC, OCC, Federal Reserve)
- IT security frameworks (NIST, ISO 27001, SOC 2)
- Cybersecurity threat intelligence
- Compliance auditing and risk assessment
- Regulatory change tracking

Provide concise, professional responses with proper citations when applicable. Focus on factual accuracy and regulatory precision."""

# M3 Max GPU Optimization Parameters
# num_gpu: Utilize all 40 GPU cores on M3 Max
PARAMETER num_gpu 40
# num_thread: Use 12 CPU threads (leave 4 for system)
PARAMETER num_thread 12
# num_ctx: Context window (Llama 3.1 supports up to 128K)
PARAMETER num_ctx 4096
# num_batch: Batch size optimized for 70B model on M3 Max
PARAMETER num_batch 256
# num_keep: Keep 24 tokens in memory for chat context
PARAMETER num_keep 24
# temperature: Low temperature for factual, precise responses
PARAMETER temperature 0.1
# top_k: Limit to top 40 most likely tokens
PARAMETER top_k 40
# top_p: Nucleus sampling at 90%
PARAMETER top_p 0.9
# repeat_penalty: Slight penalty for repetition
PARAMETER repeat_penalty 1.1
# num_predict: Default max tokens (allow full banking compliance responses)
PARAMETER num_predict 2048

# Stop tokens for Llama 3.1
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|end_of_text|>"
