# Local development environment configuration for Internal Assistant
# 
# DEPENDENCY COMPATIBILITY NOTICE:
# This environment requires specific dependency versions to ensure compatibility:
# - FastAPI: >=0.108.0,<0.115.0 (avoids Pydantic schema generation issues)
# - Pydantic: >=2.8.0,<2.9.0 (compatible with LlamaIndex)
# - Gradio: >=4.15.0,<4.39.0 (avoids FastAPI integration issues)
# - Python: 3.11.9 (required for all dependencies)
#
# Installation command:
# poetry install --extras "ui llms-llama-cpp vector-stores-qdrant embeddings-huggingface"
server:
  env_name: ${APP_ENV:local}

llm:
  mode: llamacpp
  # Should be matching the selected model
  max_new_tokens: 1024
  context_window: 8192
  # tokenizer: local/Llama-3.1-70B  # Optional: specify custom tokenizer
  prompt_style: "llama2"

llamacpp:
  llm_hf_repo_id: local/Llama-3.1-70B
  llm_hf_model_file: llama31-70b-m3max.gguf

embedding:
  mode: huggingface

huggingface:
  embedding_hf_model_name: nomic-ai/nomic-embed-text-v1.5

vectorstore:
  database: qdrant

qdrant:
  path: local_data/internal_assistant/qdrant
