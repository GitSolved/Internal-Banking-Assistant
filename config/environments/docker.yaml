server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8080}

llm:
  mode: ${PGPT_MODE:ollama}
  prompt_style: "llama3"
  max_new_tokens: 2048
  context_window: 4096
  temperature: 0.1

embedding:
  mode: ${PGPT_EMBED_MODE:huggingface}
  embed_dim: 768

vectorstore:
  database: qdrant

qdrant:
  path: local_data/internal_assistant/qdrant

rag:
  similarity_top_k: 8
  similarity_value: 0.1
  rerank:
    enabled: false

llamacpp:
  llm_hf_repo_id: ${PGPT_HF_REPO_ID:local/Llama-3.1-70B}
  llm_hf_model_file: ${PGPT_HF_MODEL_FILE:llama31-70b-m3max.gguf}

huggingface:
  embedding_hf_model_name: ${PGPT_EMBEDDING_HF_MODEL_NAME:nomic-ai/nomic-embed-text-v1.5}

sagemaker:
  llm_endpoint_name: ${PGPT_SAGEMAKER_LLM_ENDPOINT_NAME:}
  embedding_endpoint_name: ${PGPT_SAGEMAKER_EMBEDDING_ENDPOINT_NAME:}

ollama:
  llm_model: ${PGPT_OLLAMA_LLM_MODEL:llama31-70b-m3max}
  embedding_model: ${PGPT_OLLAMA_EMBEDDING_MODEL:nomic-embed-text}
  api_base: ${PGPT_OLLAMA_API_BASE:http://ollama:11434}
  embedding_api_base: ${PGPT_OLLAMA_EMBEDDING_API_BASE:http://ollama:11434}
  keep_alive: 5m
  request_timeout: ${PGPT_OLLAMA_REQUEST_TIMEOUT:180.0}
  num_predict: 16384
  autopull_models: ${PGPT_OLLAMA_AUTOPULL_MODELS:false}

ui:
  enabled: true
  path: /
