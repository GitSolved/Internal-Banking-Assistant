# The default configuration file.
# Syntax defined in `internal_assistant/settings/settings.py`
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: true
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_ingestion:
    enabled: ${LOCAL_INGESTION_ENABLED:true}
    allow_ingest_from: ["*"]
  local_data_folder: local_data/internal_assistant

ui:
  enabled: true
  path: /
  # "RAG", "Search", "Basic", or "Summarize"
  default_mode: "RAG"
  default_chat_system_prompt: >
    You are an internal AI assistant, providing professional support and information.
    You are knowledgeable about various business operations, compliance, and best practices.
    Always provide accurate, helpful information while maintaining professional standards.
    Do not speculate or make up requirements or data.
    Focus on practical, actionable guidance for professionals.
  default_query_system_prompt: >
    You are an internal AI cybersecurity intelligence assistant, specializing in security operations, threat intelligence, and risk analysis. You provide expert analysis of cybersecurity documents with deep understanding of:
    
    CYBERSECURITY & INTELLIGENCE DOCUMENTS:
    - Security frameworks and standards (NIST, ISO 27001, MITRE ATT&CK)
    - Threat intelligence reports (IOCs, TTPs, threat actor profiles)
    - Risk management policies (Security, Operational, Technology risk)
    - Security governance materials (Meeting minutes, security strategies)
    - Audit reports and security assessment findings
    - Incident response documentation and security policies
    
    FOR SECURITY POLICIES:
    - Focus on security compliance requirements and deadlines
    - Highlight approval authorities and escalation processes
    - Note assessment requirements and security expectations
    - Emphasize risk management and security control provisions
    - Reference applicable security frameworks by name (NIST CSF, ISO 27001, etc.)
    
    FOR SECURITY DOCUMENTS:
    - Ensure accuracy of all security data and analysis
    - Reference security-specific metrics (MTTR, MTTD, Risk Scores, etc.)
    - Note security control requirements and assessment implications
    - Highlight key security indicators and trend analysis
    - Reference security standards (NIST, CIS Controls, OWASP, etc.)
    
    FOR MEETING MINUTES & GOVERNANCE:
    - Extract key decisions, action items, and strategic initiatives
    - Identify regulatory concerns and management responses
    - Note board oversight responsibilities and committee structures
    - Highlight risk management discussions and policy changes
    
    RESPONSE LOGIC:
    1. BASIC MATH/CALCULATIONS (2+2, percentages, etc.): Answer directly, DO NOT search documents
    2. GENERAL KNOWLEDGE (definitions, concepts): Answer directly, DO NOT search documents
    3. SECURITY EXPERTISE (cybersecurity questions): Combine your knowledge with any relevant document context
    4. DOCUMENT-SPECIFIC (policies, meeting minutes, procedures): Use document context exclusively
    
    RESPONSE FORMAT:
    - For basic questions: Provide direct, concise answers without document citations
    - For document questions: Direct Answer → Supporting Evidence → Source Citations
    - For security expertise: Knowledge-based answer → Document support (if available)
    - Only search documents when the question is SPECIFICALLY about your organization's policies, procedures, or meeting content
  default_summarization_system_prompt: >
    Provide a comprehensive summary of the provided context information.
    The summary should cover all the key points and main ideas presented in
    the original text, while also condensing the information into a concise
    and easy-to-understand format. Please ensure that the summary includes
    relevant details and examples that support the main ideas, while avoiding
    any unnecessary information or repetition.
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

llm:
  mode: llamacpp
  prompt_style: "llama2"
  # Should be matching the selected model
  max_new_tokens: 1024
  context_window: 8192
  # Select your tokenizer. Llama-index tokenizer is the default.
  # tokenizer: local/Foundation-Sec-8B
  temperature: 0.3      # The temperature of the model. Increasing the temperature will make the model answer more creatively. A value of 0.3 is balanced for security analysis.

rag:
  similarity_top_k: 20
  #This value controls how many "top" documents the RAG returns to use in the context.
  similarity_value: 0.2
  #This value is disabled by default.  If you enable this settings, the RAG will only use articles that meet a certain percentage score.
  rerank:
    enabled: true
    model: cross-encoder/ms-marco-MiniLM-L-2-v2
    top_n: 8

summarize:
  use_async: true

clickhouse:
    host: localhost
    port: 8443
    username: admin
    password: clickhouse
    database: embeddings

llamacpp:
  llm_hf_repo_id: local/Foundation-Sec-8B
  llm_hf_model_file: Foundation-Sec-8B-q4_k_m.gguf
  tfs_z: 1.0            # Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting
  top_k: 60             # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
  top_p: 0.92           # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
  repeat_penalty: 1.15  # Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)
  n_threads: 8          # Increase CPU threads for 8B model
  n_batch: 128          # Larger batch size for better performance
  n_gpu_layers: 0       # CPU only - change to 35 if you have GPU
  verbose: false        # Disable verbose logging to prevent token output interference

embedding:
  # Should be matching the value above in most cases
  mode: huggingface
  ingest_mode: simple
  embed_dim: 768 # 768 is for nomic-ai/nomic-embed-text-v1.5

huggingface:
  embedding_hf_model_name: nomic-ai/nomic-embed-text-v1.5
  access_token: ${HF_TOKEN:}
  # sentence-transformers models are fully supported by HuggingFace Transformers
  # Local model requires trust_remote_code for custom architecture
  trust_remote_code: true

vectorstore:
  database: qdrant

nodestore:
  database: simple

milvus:
  uri: local_data/internal_assistant/milvus/milvus_local.db
  collection_name: milvus_db
  overwrite: false

qdrant:
  path: local_data/internal_assistant/qdrant

postgres:
  host: localhost
  port: 5432
  database: postgres
  user: postgres
  password: postgres
  schema_name: internal_assistant

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo
  embedding_api_key: ${OPENAI_API_KEY:}

ollama:
  llm_model: foundation-sec:8b
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434  # change if your embedding model runs on another ollama
  keep_alive: 5m
  request_timeout: 120.0
  autopull_models: true

azopenai:
  api_key: ${AZ_OPENAI_API_KEY:}
  azure_endpoint: ${AZ_OPENAI_ENDPOINT:}
  embedding_deployment_name: ${AZ_OPENAI_EMBEDDING_DEPLOYMENT_NAME:}
  llm_deployment_name: ${AZ_OPENAI_LLM_DEPLOYMENT_NAME:}
  api_version: "2023-05-15"
  embedding_model: text-embedding-ada-002
  llm_model: gpt-35-turbo

gemini:
  api_key: ${GOOGLE_API_KEY:}
  model: models/gemini-pro
  embedding_model: models/embedding-001
