# =============================================================================
# CURRENT SETUP SUMMARY (for AI agents):
# =============================================================================
# 
# ACTIVE COMPONENTS (currently used):
# - LLM: Ollama with Foundation-Sec-8B-q4_k_m.gguf (llama-index-llms-ollama)
# - Embeddings: HuggingFace nomic-embed-text-v1.5 (llama-index-embeddings-huggingface)
# - Vector Store: Qdrant (llama-index-vector-stores-qdrant)
# - Storage: Simple nodestore (built into llama-index)
# - UI: Gradio web interface (gradio, ffmpy)
# - Reranking: DISABLED (rerank.enabled: false in settings.yaml)
#
# USED BUT NOT PRIMARY:
# - llama-cpp: Model file management and settings
# - openai-like: API compatibility layer
#
# NOT USED (available for future use):
# - All other vector stores (Chroma, Postgres, ClickHouse, Milvus)
# - All other LLMs (OpenAI, Azure, Gemini, Sagemaker)
# - All other embeddings (OpenAI, Azure, Gemini, Sagemaker, Ollama)
# - PostgreSQL storage (using simple nodestore instead)
# - Reranking (disabled for performance)
# =============================================================================

[tool.poetry]
name = "internal-assistant"
version = "0.6.2"
description = "Internal Cybersecurity Intelligence Platform built on PrivateGPT technology"
authors = []

[tool.poetry.dependencies]
python = "3.11.9"
# PrivateGPT
fastapi = { extras = ["all"], version = ">=0.108.0,<0.115.0" }
python-multipart = "^0.0.10"
injector = "^0.22.0"
pyyaml = "^6.0.2"
watchdog = "^4.0.1"
transformers = "^4.44.2"
docx2txt = "^0.8"
cryptography = "^3.1"
# LlamaIndex core libs
llama-index-core = ">=0.11.2,<0.12.0"
llama-index-readers-file = "*"
# Optional LlamaIndex integration libs
llama-index-llms-llama-cpp = {version = "*", optional = true}  # ← USED: Model file management and settings
# llama-index-llms-openai = {version ="*", optional = true}  # ← NOT USED: OpenAI direct API (use openai-like instead)
llama-index-llms-openai-like = {version ="*", optional = true}  # ← USED: OpenAI-compatible API support
llama-index-llms-ollama = {version ="*", optional = true}  # ← CURRENT: Foundation-Sec-8B via Ollama
# llama-index-llms-azure-openai = {version ="*", optional = true}  # ← NOT USED: Azure OpenAI (not configured)
# llama-index-llms-gemini = {version ="*", optional = true}  # ← NOT USED: Google Gemini (not configured)
# llama-index-embeddings-ollama = {version ="*", optional = true}  # ← NOT USED: Ollama embeddings (use HuggingFace instead)
llama-index-embeddings-huggingface = {version ="*", optional = true}  # ← CURRENT: nomic-embed-text-v1.5
# llama-index-embeddings-openai = {version ="*", optional = true}  # ← NOT USED: OpenAI embeddings (use HuggingFace instead)
# llama-index-embeddings-azure-openai = {version ="*", optional = true}  # ← NOT USED: Azure embeddings (not configured)
# llama-index-embeddings-gemini = {version ="*", optional = true}  # ← NOT USED: Google embeddings (not configured)
llama-index-vector-stores-qdrant = {version ="*", optional = true}  # ← CURRENT: Qdrant vector store
llama-index-vector-stores-milvus = {version ="*", optional = true}  # ← NOT USED: Milvus vector store (use Qdrant instead)
llama-index-vector-stores-chroma = {version ="*", optional = true}  # ← NOT USED: ChromaDB vector store (use Qdrant instead)
llama-index-vector-stores-postgres = {version ="*", optional = true}  # ← NOT USED: PostgreSQL vector store (use Qdrant instead)
llama-index-vector-stores-clickhouse = {version ="*", optional = true}  # ← NOT USED: ClickHouse vector store (use Qdrant instead)
llama-index-storage-docstore-postgres = {version ="*", optional = true}  # ← NOT USED: PostgreSQL storage (use simple nodestore instead)
llama-index-storage-index-store-postgres = {version ="*", optional = true}  # ← NOT USED: PostgreSQL index store (use simple nodestore instead)
# Postgres dependencies (for PostgreSQL storage - NOT USED)
psycopg2-binary = {version ="^2.9.9", optional = true}  # ← NOT USED: PostgreSQL adapter (use simple nodestore instead)
asyncpg = {version="^0.29.0", optional = true}  # ← NOT USED: Async PostgreSQL (use simple nodestore instead)

# ClickHouse dependencies (for ClickHouse vector store - NOT USED)
clickhouse-connect = {version = "^0.7.19", optional = true}  # ← NOT USED: ClickHouse connector (use Qdrant instead)

# Sagemaker dependencies (for AWS Sagemaker - NOT USED)
boto3 = {version ="^1.35.26", optional = true}  # ← NOT USED: AWS SDK (not configured for Sagemaker)

# Reranker dependencies (currently disabled in settings)
torch = {version ="^2.4.1", optional = true}  # ← NOT USED: PyTorch (reranking disabled)
sentence-transformers = {version ="^3.1.1", optional = true}  # ← NOT USED: Sentence transformers (reranking disabled)

# UI dependencies (REQUIRED for Gradio interface)
gradio = {version =">=4.15.0,<4.39.0", optional = true}  # ← CURRENT: Gradio web interface
ffmpy = {version ="^0.4.0", optional = true}  # ← CURRENT: Media processing for UI

# RSS feed dependencies
feedparser = "^6.0.10"
aiohttp = "^3.9.0"
beautifulsoup4 = "^4.12.0"

# Optional HF Transformers
einops = {version = "^0.8.0", optional = true}
retry-async = "^0.1.4"
pydantic = ">=2.8.0,<2.9.0"
[tool.poetry.extras]
# UI extras (REQUIRED for Gradio interface)
ui = ["gradio", "ffmpy"]  # ← CURRENT: Gradio web interface

# LLM extras
llms-llama-cpp = ["llama-index-llms-llama-cpp"]  # ← USED: Model file management and settings
# llms-openai = ["llama-index-llms-openai"]  # ← NOT USED: OpenAI direct API (use openai-like instead)
llms-openai-like = ["llama-index-llms-openai-like"]  # ← USED: OpenAI-compatible API support
llms-ollama = ["llama-index-llms-ollama"]  # ← CURRENT: Foundation-Sec-8B via Ollama
llms-sagemaker = ["boto3"]  # ← NOT USED: AWS Sagemaker (not configured)
# llms-azopenai = ["llama-index-llms-azure-openai"]  # ← NOT USED: Azure OpenAI (not configured)
# llms-gemini = ["llama-index-llms-gemini"]  # ← NOT USED: Google Gemini (not configured)

# Embedding extras
# embeddings-ollama = ["llama-index-embeddings-ollama"]  # ← NOT USED: Ollama embeddings (use HuggingFace instead)
embeddings-huggingface = ["llama-index-embeddings-huggingface", "einops"]  # ← CURRENT: nomic-embed-text-v1.5
# embeddings-openai = ["llama-index-embeddings-openai"]  # ← NOT USED: OpenAI embeddings (use HuggingFace instead)
embeddings-sagemaker = ["boto3"]  # ← NOT USED: AWS Sagemaker embeddings (not configured)
# embeddings-azopenai = ["llama-index-embeddings-azure-openai"]  # ← NOT USED: Azure embeddings (not configured)
# embeddings-gemini = ["llama-index-embeddings-gemini"]  # ← NOT USED: Google embeddings (not configured)
# embeddings-mistral = ["llama-index-embeddings-mistralai"]  # ← NOT USED: Mistral embeddings (not configured)

# Vector store extras
vector-stores-qdrant = ["llama-index-vector-stores-qdrant"]  # ← CURRENT: Qdrant vector store
vector-stores-clickhouse = ["llama-index-vector-stores-clickhouse", "clickhouse-connect"]  # ← NOT USED: ClickHouse vector store (use Qdrant instead)
vector-stores-chroma = ["llama-index-vector-stores-chroma"]  # ← NOT USED: ChromaDB vector store (use Qdrant instead)
vector-stores-postgres = ["llama-index-vector-stores-postgres"]  # ← NOT USED: PostgreSQL vector store (use Qdrant instead)
vector-stores-milvus = ["llama-index-vector-stores-milvus"]  # ← NOT USED: Milvus vector store (use Qdrant instead)

# Storage extras
storage-nodestore-postgres = ["llama-index-storage-docstore-postgres","llama-index-storage-index-store-postgres","psycopg2-binary","asyncpg"]  # ← NOT USED: PostgreSQL storage (use simple nodestore instead)

# Reranker extras (currently disabled in settings)
rerank-sentence-transformers = ["torch", "sentence-transformers"]  # ← NOT USED: RAG reranking (disabled in settings.yaml)

[tool.poetry.group.dev.dependencies]
black = "^24"
mypy = "^1.11"
pre-commit = "^3"
pytest = "^8"
pytest-cov = "^5"
ruff = "^0"
pytest-asyncio = "^0.24.0"
types-pyyaml = "^6.0.12.20240917"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

# Packages configs

## coverage

[tool.coverage.run]
branch = true

[tool.coverage.report]
skip_empty = true
precision = 2

## black

[tool.black]
target-version = ['py311']

## ruff
# Recommended ruff config for now, to be updated as we go along.
[tool.ruff]
target-version = 'py311'

# See all rules at https://beta.ruff.rs/docs/rules/
lint.select = [
    "E", # pycodestyle
    "W", # pycodestyle
    "F", # Pyflakes
    "B", # flake8-bugbear
    "C4", # flake8-comprehensions
    "D", # pydocstyle
    "I", # isort
    "SIM", # flake8-simplify
    "TCH", # flake8-type-checking
    "TID", # flake8-tidy-imports
    "Q", # flake8-quotes
    "UP", # pyupgrade
    "PT", # flake8-pytest-style
    "RUF", # Ruff-specific rules
]

lint.ignore = [
    "E501", # "Line too long"
    # -> line length already regulated by black
    "PT011", # "pytest.raises() should specify expected exception"
    # -> would imply to update tests every time you update exception message
    "SIM102", # "Use a single `if` statement instead of nested `if` statements"
    # -> too restrictive,
    "D100",
    "D101",
    "D102",
    "D103",
    "D104",
    "D105",
    "D106",
    "D107"
    # -> "Missing docstring in public function too restrictive"
]

[tool.ruff.lint.pydocstyle]
# Automatically disable rules that are incompatible with Google docstring convention
convention = "google"

[tool.ruff.lint.pycodestyle]
max-doc-length = 88

[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = "all"

[tool.ruff.lint.flake8-type-checking]
strict = true
runtime-evaluated-base-classes = ["pydantic.BaseModel"]
# Pydantic needs to be able to evaluate types at runtime
# see https://pypi.org/project/flake8-type-checking/ for flake8-type-checking documentation
# see https://beta.ruff.rs/docs/settings/#flake8-type-checking-runtime-evaluated-base-classes for ruff documentation

[tool.ruff.lint.per-file-ignores]
# Allow missing docstrings for tests
"tests/**/*.py" = ["D1"]

## mypy

[tool.mypy]
python_version = "3.11"
strict = true
check_untyped_defs = false
explicit_package_bases = true
warn_unused_ignores = false
exclude = ["tests"]

[tool.mypy-llama-index]
ignore_missing_imports = true

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]
addopts = [
    "--import-mode=importlib",
]
# Logging configuration
log_cli = true
log_cli_level = "INFO"
log_cli_format = "%(asctime)s [%(levelname)8s] %(name)s - %(message)s"
log_cli_date_format = "%Y-%m-%d %H:%M:%S"
log_file = "Logs/pytest.log"
log_file_level = "DEBUG"
log_file_format = "%(asctime)s [%(levelname)8s] %(name)s - %(message)s"
log_file_date_format = "%Y-%m-%d %H:%M:%S"
