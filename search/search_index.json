{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Internal Assistant Documentation","text":"<p>Welcome to the Internal Assistant documentation. This guide will help you install, configure, and use the cybersecurity intelligence platform.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>New Users: Start with the Installation Guide to get Internal Assistant running on your system.</p> <p>Developers: Check out the Development Setup to contribute to the project.</p> <p>API Users: See the API Reference for endpoint documentation.</p> <p>Troubleshooting: Visit the Troubleshooting Guide if you encounter issues.</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#user-documentation","title":"User Documentation","text":"<p>Guides for end users to install, configure, and use Internal Assistant.</p> <ul> <li>Installation - Step-by-step installation instructions</li> <li>Concepts - Core concepts and architecture</li> <li>Configuration - Settings and profiles</li> <li>LLM Configuration - Model configuration options</li> <li>Quick Start - Get started quickly</li> <li>Document Ingestion - Manage documents</li> <li>Ingestion Reset - Clear ingested documents</li> <li>Summarization - Document summarization features</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"#api-documentation","title":"API Documentation","text":"<p>Technical reference for API endpoints and integration.</p> <ul> <li>API Reference - REST API endpoint documentation</li> </ul>"},{"location":"#developer-documentation","title":"Developer Documentation","text":"<p>Guides for contributors and developers working on Internal Assistant.</p> <ul> <li>Architecture Overview - System architecture and design</li> <li>Data Lifecycle - Data management strategy</li> <li>Development Setup - Setting up your dev environment</li> <li>Package Structure - Code organization and imports</li> <li>Documentation Guidelines - Documentation standards</li> </ul>"},{"location":"#learning-path","title":"Learning Path","text":""},{"location":"#for-new-users","title":"For New Users","text":"<ol> <li>Read Concepts to understand the platform</li> <li>Follow the Installation Guide</li> <li>Configure your setup using Configuration</li> <li>Try the Quick Start examples</li> </ol>"},{"location":"#for-developers","title":"For Developers","text":"<ol> <li>Review the Architecture Overview</li> <li>Set up your environment with Development Setup</li> <li>Understand the Package Structure</li> </ol>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Issues: Report bugs or request features on the project repository</li> <li>Questions: Check the troubleshooting guide or open a discussion</li> <li>Contributions: See the development documentation for contribution guidelines</li> </ul>"},{"location":"api/reference/api-reference/","title":"API Reference","text":"<p>The API is divided in two logical blocks:</p> <ol> <li> <p>High-level API, abstracting all the complexity of a RAG (Retrieval Augmented Generation) pipeline implementation:</p> <ul> <li>Ingestion of documents: internally managing document parsing, splitting, metadata extraction,   embedding generation and storage.</li> <li>Chat &amp; Completions using context from ingested documents: abstracting the retrieval of context, the prompt   engineering and the response generation.</li> </ul> </li> <li> <p>Low-level API, allowing advanced users to implement their own complex pipelines:</p> <ul> <li>Embeddings generation: based on a piece of text.</li> <li>Contextual chunks retrieval: given a query, returns the most relevant chunks of text from the ingested   documents.</li> </ul> </li> </ol>"},{"location":"api/reference/api-reference/#api-endpoints","title":"API Endpoints","text":""},{"location":"api/reference/api-reference/#high-level-api","title":"High-Level API","text":""},{"location":"api/reference/api-reference/#chat-endpoints","title":"Chat Endpoints","text":"<ul> <li><code>POST /v1/chat/completions</code> - Generate chat completions with context from ingested documents</li> <li><code>POST /v1/chat/completions/stream</code> - Stream chat completions</li> </ul>"},{"location":"api/reference/api-reference/#ingestion-endpoints","title":"Ingestion Endpoints","text":"<ul> <li><code>POST /v1/ingest/file</code> - Ingest a single file</li> <li><code>POST /v1/ingest/files</code> - Ingest multiple files</li> <li><code>POST /v1/ingest/url</code> - Ingest content from a URL</li> <li><code>DELETE /v1/ingest/{doc_id}</code> - Delete a specific document</li> <li><code>DELETE /v1/ingest</code> - Delete all documents</li> </ul>"},{"location":"api/reference/api-reference/#low-level-api","title":"Low-Level API","text":""},{"location":"api/reference/api-reference/#embeddings","title":"Embeddings","text":"<ul> <li><code>POST /v1/embeddings</code> - Generate embeddings for text</li> </ul>"},{"location":"api/reference/api-reference/#chunks","title":"Chunks","text":"<ul> <li><code>GET /v1/chunks</code> - Retrieve contextual chunks based on query</li> </ul>"},{"location":"api/reference/api-reference/#authentication","title":"Authentication","text":"<p>The API uses API key authentication. Include your API key in the request headers:</p> <pre><code>Authorization: Bearer your-api-key-here\n</code></pre>"},{"location":"api/reference/api-reference/#rate-limits","title":"Rate Limits","text":"<p>Rate limits can be configured in the settings file based on your deployment requirements.</p>"},{"location":"api/reference/api-reference/#error-handling","title":"Error Handling","text":"<p>The API returns standard HTTP status codes:</p> <ul> <li><code>200</code> - Success</li> <li><code>400</code> - Bad Request</li> <li><code>401</code> - Unauthorized</li> <li><code>403</code> - Forbidden</li> <li><code>404</code> - Not Found</li> <li><code>429</code> - Rate Limited</li> <li><code>500</code> - Internal Server Error</li> </ul>"},{"location":"api/reference/api-reference/#response-format","title":"Response Format","text":"<p>All successful responses follow this format:</p> <pre><code>{\n  \"data\": {\n    // Response data\n  },\n  \"meta\": {\n    \"timestamp\": \"2024-01-01T00:00:00Z\",\n    \"request_id\": \"req_123456789\"\n  }\n}\n</code></pre>"},{"location":"api/reference/api-reference/#client-libraries","title":"Client Libraries","text":"<p>Use any OpenAI-compatible client library to interact with the API. The API follows OpenAI's specification for compatibility.</p>"},{"location":"api/reference/api-reference/#examples","title":"Examples","text":""},{"location":"api/reference/api-reference/#chat-completion","title":"Chat Completion","text":"<pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8001/v1/chat/completions\",\n    headers={\n        \"Authorization\": \"Bearer your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What are the main features of Internal Assistant?\"}\n        ],\n        \"stream\": False\n    }\n)\n\nprint(response.json())\n</code></pre>"},{"location":"api/reference/api-reference/#document-ingestion","title":"Document Ingestion","text":"<pre><code>import requests\n\nwith open(\"document.pdf\", \"rb\") as f:\n    files = {\"file\": f}\n    response = requests.post(\n        \"http://localhost:8001/v1/ingest/file\",\n        headers={\"Authorization\": \"Bearer your-api-key\"},\n        files=files\n    )\n\nprint(response.json())\n</code></pre>"},{"location":"api/reference/api-reference/#additional-resources","title":"Additional Resources","text":"<ul> <li>Quick Start Guide - Get started with Internal Assistant</li> <li>Configuration - Configure the API</li> <li>Developer Setup - Set up development environment</li> </ul>"},{"location":"compatibility/compatibility_guide/","title":"Internal Assistant - Compatibility Guide","text":""},{"location":"compatibility/compatibility_guide/#overview","title":"Overview","text":"<p>This document provides compatibility information for Internal Assistant. All version requirements are validated by the version check system in <code>internal_assistant/utils/version_check.py</code>.</p>"},{"location":"compatibility/compatibility_guide/#application-version","title":"Application Version","text":"<ul> <li>Current Version: 0.6.2</li> </ul>"},{"location":"compatibility/compatibility_guide/#python-version","title":"Python Version","text":"<ul> <li>Required: 3.11.9 (exact)</li> <li>Status: \u2705 Compatible</li> </ul>"},{"location":"compatibility/compatibility_guide/#critical-dependencies","title":"Critical Dependencies","text":"<p>These packages are essential for the application to function:</p>"},{"location":"compatibility/compatibility_guide/#fastapi","title":"fastapi","text":"<ul> <li>Required Range: &gt;=0.108.0, &lt;0.115.0</li> <li>Status: \u2705 Compatible</li> </ul>"},{"location":"compatibility/compatibility_guide/#pydantic","title":"pydantic","text":"<ul> <li>Required Range: &gt;=2.8.0, &lt;2.9.0</li> <li>Status: \u2705 Compatible</li> </ul>"},{"location":"compatibility/compatibility_guide/#llama-index-core","title":"llama-index-core","text":"<ul> <li>Required Range: &gt;=0.11.2, &lt;0.12.0</li> <li>Status: \u2705 Compatible</li> </ul>"},{"location":"compatibility/compatibility_guide/#gradio","title":"gradio","text":"<ul> <li>Required Range: &gt;=4.15.0, &lt;4.39.0</li> <li>Status: \u2705 Compatible</li> </ul>"},{"location":"compatibility/compatibility_guide/#transformers","title":"transformers","text":"<ul> <li>Required Range: &gt;=4.44.2, &lt;5.0.0</li> <li>Status: \u2705 Compatible</li> </ul>"},{"location":"compatibility/compatibility_guide/#all-dependencies","title":"All Dependencies","text":"<p>Complete list of all dependencies and their compatibility status:</p>"},{"location":"compatibility/compatibility_guide/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>fastapi: &gt;=0.108.0, &lt;0.115.0 \u2705</li> <li>pydantic: &gt;=2.8.0, &lt;2.9.0 \u2705</li> <li>gradio: &gt;=4.15.0, &lt;4.39.0 \u2705</li> <li>llama-index-core: &gt;=0.11.2, &lt;0.12.0 \u2705</li> <li>transformers: &gt;=4.44.2, &lt;5.0.0 \u2705</li> </ul>"},{"location":"compatibility/compatibility_guide/#llm-embeddings","title":"LLM &amp; Embeddings","text":"<ul> <li>llama-index-llms-ollama: Any version \u2705</li> <li>llama-index-llms-openai-like: Any version \u2705</li> <li>llama-index-embeddings-huggingface: Any version \u2705</li> <li>sentence-transformers: &gt;=3.1.1, &lt;4.0.0 \u2705</li> </ul>"},{"location":"compatibility/compatibility_guide/#storage-data","title":"Storage &amp; Data","text":"<ul> <li>llama-index-vector-stores-qdrant: Any version \u2705</li> <li>feedparser: &gt;=6.0.10, &lt;7.0.0 \u2705</li> <li>aiohttp: &gt;=3.9.0, &lt;4.0.0 \u2705</li> <li>beautifulsoup4: &gt;=4.12.0, &lt;5.0.0 \u2705</li> </ul>"},{"location":"compatibility/compatibility_guide/#utilities","title":"Utilities","text":"<ul> <li>cryptography: &gt;=3.1, &lt;4.0.0 \u2705</li> <li>python-multipart: &gt;=0.0.10, &lt;1.0.0 \u2705</li> <li>docx2txt: &gt;=0.8, &lt;1.0.0 \u2705</li> <li>psutil: &gt;=7.0.0, &lt;8.0.0 \u2705</li> <li>watchdog: &gt;=4.0.1, &lt;5.0.0 \u2705</li> </ul>"},{"location":"compatibility/compatibility_guide/#development-tools","title":"Development Tools","text":"<ul> <li>pytest: &gt;=8.0.0, &lt;9.0.0 \u2705</li> <li>black: &gt;=24.0.0, &lt;25.0.0 \u2705</li> <li>ruff: &gt;=0.0.0, &lt;1.0.0 \u2705</li> </ul>"},{"location":"compatibility/compatibility_guide/#system-requirements","title":"System Requirements","text":"<ul> <li>RAM: 8GB+ recommended</li> <li>Storage: 10GB+ for models and data</li> <li>Python: 3.11.9 (exact version required)</li> </ul>"},{"location":"compatibility/compatibility_guide/#validation-commands","title":"Validation Commands","text":"<p>To check compatibility:</p> <pre><code># Check dependency compatibility\nmake compatibility-check\n\n# Or directly with poetry\npoetry run python tools/system/manage_compatibility.py --check\n</code></pre>"},{"location":"compatibility/compatibility_guide/#installation","title":"Installation","text":"<p>Install with all required dependencies:</p> <pre><code>poetry install --extras \"ui llms-ollama embeddings-huggingface vector-stores-qdrant\"\n</code></pre>"},{"location":"compatibility/compatibility_guide/#notes","title":"Notes","text":"<ul> <li>All version constraints are defined in <code>internal_assistant/utils/version_check.py</code></li> <li>The version check system validates these constraints at startup</li> <li>Critical packages are marked for immediate attention if incompatible</li> <li>Wildcard versions indicate any version is acceptable</li> <li>See <code>pyproject.toml</code> for complete dependency specifications</li> </ul>"},{"location":"developer/architecture/data-lifecycle/","title":"Data Lifecycle Management","text":""},{"location":"developer/architecture/data-lifecycle/#overview","title":"Overview","text":"<p>This document outlines the data management strategy for the Internal Assistant platform, covering data organization, lifecycle management, backup procedures, and cleanup operations.</p>"},{"location":"developer/architecture/data-lifecycle/#data-organization","title":"Data Organization","text":""},{"location":"developer/architecture/data-lifecycle/#primary-data-locations","title":"Primary Data Locations","text":""},{"location":"developer/architecture/data-lifecycle/#local_data-main-application-data","title":"<code>local_data/</code> - Main Application Data","text":"<ul> <li>Purpose: Primary runtime and persistent data storage</li> <li>Structure:   <pre><code>local_data/\n\u251c\u2500\u2500 logs/                    # Application logs\n\u251c\u2500\u2500 internal_assistant/      # Vector database, app state\n\u251c\u2500\u2500 models/                  # AI model files (consolidated)\n\u2514\u2500\u2500 cache/                   # Token encoding cache (consolidated)\n</code></pre></li> </ul>"},{"location":"developer/architecture/data-lifecycle/#data-legacy-storage-deprecated","title":"<code>data/</code> - Legacy Storage (Deprecated)","text":"<ul> <li>Status: LEGACY LOCATION - Migration in progress</li> <li>Purpose: Previously used for persistent user data</li> <li>Action: Migrate to <code>local_data/</code> structure</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#cache-management","title":"Cache Management","text":""},{"location":"developer/architecture/data-lifecycle/#token-encoding-cache","title":"Token Encoding Cache","text":"<ul> <li>Location: <code>local_data/cache/</code> (consolidated from root <code>tiktoken_cache/</code>)</li> <li>Purpose: Stores tokenizer cache for improved performance</li> <li>Configuration: </li> <li>Environment Variable: <code>TIKTOKEN_CACHE_DIR=local_data/cache/</code></li> <li>Default: Automatically set to consolidated location</li> <li>Migration: Completed - old root location removed</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#model-cache","title":"Model Cache","text":"<ul> <li>Location: <code>local_data/models/cache/</code></li> <li>Purpose: HuggingFace model cache and downloaded models</li> <li>Size: 5GB+ for Foundation-Sec-8B model</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#log-management","title":"Log Management","text":""},{"location":"developer/architecture/data-lifecycle/#application-logs","title":"Application Logs","text":"<ul> <li>Location: <code>local_data/logs/</code></li> <li>Types:</li> <li>Application logs</li> <li>Error logs</li> <li>Access logs</li> <li>Debug logs</li> <li>Retention: Configurable via settings</li> <li>Cleanup: Automated via maintenance scripts</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#data-lifecycle","title":"Data Lifecycle","text":""},{"location":"developer/architecture/data-lifecycle/#1-creation-phase","title":"1. Creation Phase","text":"<ul> <li>User uploads documents</li> <li>System processes and indexes content</li> <li>Vector embeddings generated</li> <li>Metadata stored</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#2-storage-phase","title":"2. Storage Phase","text":"<ul> <li>Data stored in <code>local_data/internal_assistant/</code></li> <li>Vector database maintains embeddings</li> <li>Document metadata tracked</li> <li>Cache files generated for performance</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#3-access-phase","title":"3. Access Phase","text":"<ul> <li>Users query the system</li> <li>Vector search performed</li> <li>Results returned with context</li> <li>Cache utilized for performance</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#4-maintenance-phase","title":"4. Maintenance Phase","text":"<ul> <li>Regular cleanup of old logs</li> <li>Cache optimization</li> <li>Database maintenance</li> <li>Performance monitoring</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#5-backup-phase","title":"5. Backup Phase","text":"<ul> <li>Regular backups of critical data</li> <li>Configuration backups</li> <li>Model file backups (if custom)</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#backup-strategy","title":"Backup Strategy","text":""},{"location":"developer/architecture/data-lifecycle/#critical-data-to-backup","title":"Critical Data to Backup","text":"<ol> <li>Vector Database: <code>local_data/internal_assistant/</code></li> <li>Configuration: <code>config/</code> directory</li> <li>Custom Models: <code>local_data/models/</code> (if custom)</li> <li>User Documents: <code>local_data/internal_assistant/documents/</code></li> </ol>"},{"location":"developer/architecture/data-lifecycle/#backup-frequency","title":"Backup Frequency","text":"<ul> <li>Daily: Vector database and configuration</li> <li>Weekly: Full system backup</li> <li>Monthly: Complete data archive</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#backup-commands","title":"Backup Commands","text":"<pre><code># Manual backup using standard tools\ntar -czf backup-$(date +%Y%m%d).tar.gz local_data/ config/\n\n# Or use rsync for incremental backups\nrsync -av --progress local_data/ /path/to/backup/local_data/\nrsync -av --progress config/ /path/to/backup/config/\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#cleanup-operations","title":"Cleanup Operations","text":""},{"location":"developer/architecture/data-lifecycle/#available-maintenance-tools","title":"Available Maintenance Tools","text":"<pre><code># Manage application logs\npoetry run python tools/maintenance/manage_logs.py --cleanup\n\n# Control logging levels\npoetry run python tools/maintenance/logging_control.py\n\n# Analyze model files and storage\npoetry run python tools/maintenance/analyze_models.py\n\n# Clean up unused model files\npoetry run python tools/maintenance/cleanup_unused_models.py\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#manual-cleanup","title":"Manual Cleanup","text":"<pre><code># Remove all cached data (if cache directory exists)\nrm -rf local_data/cache/*\n\n# Remove old logs\nrm -rf local_data/logs/*.log\n\n# Reset vector database (use with caution)\nrm -rf local_data/internal_assistant/qdrant/\n\n# Remove Qdrant lock files (if database won't start)\nrm -f local_data/internal_assistant/qdrant/.lock\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#docker-deployment","title":"Docker Deployment","text":""},{"location":"developer/architecture/data-lifecycle/#data-persistence","title":"Data Persistence","text":"<pre><code># docker-compose.yaml\nvolumes:\n  - ./local_data:/app/local_data\n  - ./config:/app/config\n  - ./models:/app/local_data/models\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#environment-variables","title":"Environment Variables","text":"<pre><code># Data paths\nTIKTOKEN_CACHE_DIR=/app/local_data/cache\nPGPT_SETTINGS_FOLDER=/app/config\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":""},{"location":"developer/architecture/data-lifecycle/#health-checks","title":"Health Checks","text":"<pre><code># Check application health\ncurl http://localhost:8001/health\n\n# Check disk usage\ndu -sh local_data/*\n\n# Analyze model files\npoetry run python tools/maintenance/analyze_models.py\n\n# Check logs\ntail -f local_data/logs/*.log\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#performance-monitoring","title":"Performance Monitoring","text":"<ul> <li>Monitor vector database size with <code>du -sh local_data/internal_assistant/qdrant/</code></li> <li>Check log file growth with <code>ls -lh local_data/logs/</code></li> <li>Analyze model storage with the analyze_models.py tool</li> <li>Use application health endpoint for system status</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"developer/architecture/data-lifecycle/#data-recovery","title":"Data Recovery","text":"<ol> <li>Stop the application</li> <li>Restore from backup</li> <li>Verify data integrity</li> <li>Restart application</li> </ol>"},{"location":"developer/architecture/data-lifecycle/#cache-recovery","title":"Cache Recovery","text":"<pre><code># Clear and rebuild cache\nrm -rf local_data/cache/*\npoetry run python -m internal_assistant\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#database-recovery","title":"Database Recovery","text":"<pre><code># Reset vector database (Qdrant)\nrm -rf local_data/internal_assistant/qdrant/\npoetry run python -m internal_assistant\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#best-practices","title":"Best Practices","text":""},{"location":"developer/architecture/data-lifecycle/#data-management","title":"Data Management","text":"<ul> <li>Regular backups of critical data</li> <li>Monitor disk space usage</li> <li>Implement log rotation</li> <li>Use appropriate file permissions</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#performance","title":"Performance","text":"<ul> <li>Keep cache directory on fast storage</li> <li>Regular cleanup of old data</li> <li>Monitor vector database size</li> <li>Optimize model storage</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#security","title":"Security","text":"<ul> <li>Secure backup storage</li> <li>Encrypt sensitive data</li> <li>Regular security audits</li> <li>Access control for data directories</li> </ul>"},{"location":"developer/architecture/data-lifecycle/#configuration-reference","title":"Configuration Reference","text":""},{"location":"developer/architecture/data-lifecycle/#environment-variables_1","title":"Environment Variables","text":"<pre><code># Data paths\nTIKTOKEN_CACHE_DIR=local_data/cache/\nPGPT_SETTINGS_FOLDER=config/\nPGPT_DATA_FOLDER=local_data/\n\n# Logging\nPGPT_LOG_LEVEL=INFO\nPGPT_LOG_FILE=local_data/logs/app.log\n\n# Cache settings\nPGPT_CACHE_TTL=3600\nPGPT_MAX_CACHE_SIZE=1GB\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#settings-configuration","title":"Settings Configuration","text":"<pre><code># config/settings.yaml\ndata:\n  local_data_folder: local_data/internal_assistant\n\n# Vector store is managed by Qdrant\n# Logs are in local_data/logs/\n# Models are cached in local_data/models/cache/\n</code></pre>"},{"location":"developer/architecture/data-lifecycle/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/architecture/data-lifecycle/#common-issues","title":"Common Issues","text":"<ol> <li>Disk Space: Monitor and clean up old data</li> <li>Cache Corruption: Clear and rebuild cache</li> <li>Database Locks: Use cleanup scripts</li> <li>Permission Issues: Check file permissions</li> </ol>"},{"location":"developer/architecture/data-lifecycle/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Check data directory structure\nfind local_data -type d -maxdepth 3\n\n# Check disk usage\ndu -sh local_data/*\n\n# Check file permissions\nls -la local_data/\n\n# Analyze models and storage\npoetry run python tools/maintenance/analyze_models.py\n</code></pre> <p>Note: This document should be updated as the data management strategy evolves. Always test backup and recovery procedures in a safe environment before implementing in production.</p>"},{"location":"developer/architecture/overview/","title":"Architecture Overview","text":"<p>Internal Assistant is a cybersecurity intelligence platform built on FastAPI and LlamaIndex, designed for local, privacy-focused threat analysis.</p> <p>Related Documentation: - Package Structure Guide - Code organization and imports - Data Lifecycle - Data management strategy - Development Setup - Getting started with development</p>"},{"location":"developer/architecture/overview/#system-architecture","title":"System Architecture","text":"<p>Internal Assistant is a cybersecurity intelligence platform evolved from open-source RAG technology, designed for secure, local document processing and threat intelligence analysis.</p>"},{"location":"developer/architecture/overview/#core-components","title":"Core Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Internal Assistant                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Web Interface (Gradio)                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Chat UI   \u2502  \u2502  Document   \u2502  \u2502   Threat    \u2502         \u2502\n\u2502  \u2502             \u2502  \u2502  Upload     \u2502  \u2502 Intelligence\u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  API Layer (FastAPI)                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Chat      \u2502  \u2502  Ingest     \u2502  \u2502   Feeds     \u2502         \u2502\n\u2502  \u2502   API       \u2502  \u2502  API        \u2502  \u2502   API       \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Core Services                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   LLM       \u2502  \u2502 Embeddings  \u2502  \u2502 Vector Store\u2502         \u2502\n\u2502  \u2502 (Ollama)    \u2502  \u2502(HuggingFace)\u2502  \u2502  (Qdrant)   \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Data Layer                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Models    \u2502  \u2502   Storage   \u2502  \u2502    Logs     \u2502         \u2502\n\u2502  \u2502 (Local)     \u2502  \u2502  (Local)    \u2502  \u2502  (Local)    \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"developer/architecture/overview/#component-details","title":"Component Details","text":""},{"location":"developer/architecture/overview/#1-web-interface-gradio","title":"1. Web Interface (Gradio)","text":"<ul> <li>Purpose: Web interface for interaction</li> <li>Technology: Gradio with custom cybersecurity theme</li> <li>Features: Chat interface, document upload, threat intelligence dashboard</li> </ul>"},{"location":"developer/architecture/overview/#2-api-layer-fastapi","title":"2. API Layer (FastAPI)","text":"<ul> <li>Purpose: RESTful API for programmatic access</li> <li>Technology: FastAPI with automatic OpenAPI documentation</li> <li>Endpoints: Chat, ingestion, feeds, health, metadata</li> </ul>"},{"location":"developer/architecture/overview/#3-core-services","title":"3. Core Services","text":""},{"location":"developer/architecture/overview/#llm-service-ollama","title":"LLM Service (Ollama)","text":"<ul> <li>Model: Foundation-Sec-8B-q4_k_m.gguf (4.7GB)</li> <li>Purpose: Cybersecurity-focused language model</li> <li>Integration: LlamaIndex LLM component</li> </ul>"},{"location":"developer/architecture/overview/#embeddings-service-huggingface","title":"Embeddings Service (HuggingFace)","text":"<ul> <li>Model: nomic-embed-text-v1.5</li> <li>Purpose: Document vectorization for similarity search</li> <li>Integration: LlamaIndex embeddings component</li> </ul>"},{"location":"developer/architecture/overview/#vector-store-qdrant","title":"Vector Store (Qdrant)","text":"<ul> <li>Purpose: Vector database</li> <li>Features: Similarity search, metadata filtering</li> <li>Storage: Local Qdrant instance</li> </ul>"},{"location":"developer/architecture/overview/#4-data-layer","title":"4. Data Layer","text":""},{"location":"developer/architecture/overview/#models","title":"Models","text":"<ul> <li>Location: <code>local_data/models/</code> directory</li> <li>Content: Foundation-Sec-8B model file (~4.7GB)</li> <li>Management: Ollama handles model storage</li> </ul>"},{"location":"developer/architecture/overview/#storage","title":"Storage","text":"<ul> <li>Location: <code>local_data/internal_assistant/</code></li> <li>Content: Vector embeddings, document metadata</li> <li>Technology: Qdrant database files</li> </ul>"},{"location":"developer/architecture/overview/#logs","title":"Logs","text":"<ul> <li>Location: <code>local_data/logs/</code></li> <li>Content: Application logs, session logs</li> <li>Management: Automatic cleanup via <code>make log-cleanup</code></li> </ul>"},{"location":"developer/architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"developer/architecture/overview/#document-processing","title":"Document Processing","text":"<pre><code>1. Document Upload \u2192 2. Text Extraction \u2192 3. Chunking \u2192 4. Embedding \u2192 5. Vector Storage\n</code></pre>"},{"location":"developer/architecture/overview/#query-processing","title":"Query Processing","text":"<pre><code>1. User Query \u2192 2. Embedding \u2192 3. Vector Search \u2192 4. Context Retrieval \u2192 5. LLM Response\n</code></pre>"},{"location":"developer/architecture/overview/#threat-intelligence","title":"Threat Intelligence","text":"<pre><code>1. RSS/Forum Feeds \u2192 2. Content Parsing \u2192 3. Threat Analysis \u2192 4. Intelligence Dashboard\n</code></pre>"},{"location":"developer/architecture/overview/#security-features","title":"Security Features","text":""},{"location":"developer/architecture/overview/#privacy-first-design","title":"Privacy-First Design","text":"<ul> <li>Local Processing: All data processed locally</li> <li>No External APIs: No data sent to external services</li> <li>Encrypted Storage: Local data encryption</li> <li>Access Control: Configurable authentication</li> </ul>"},{"location":"developer/architecture/overview/#data-isolation","title":"Data Isolation","text":"<ul> <li>Separate Environments: Development, testing, production</li> <li>Configurable Storage: Local file system or database</li> <li>Log Management: Automatic cleanup and rotation</li> </ul>"},{"location":"developer/architecture/overview/#configuration-management","title":"Configuration Management","text":""},{"location":"developer/architecture/overview/#configuration-files-phase-2a-structure","title":"Configuration Files (Phase 2A Structure)","text":"<ul> <li>Base: <code>config/settings.yaml</code> (always loaded)</li> <li>Model-specific: <code>config/model-configs/foundation-sec.yaml</code>, <code>ollama.yaml</code></li> <li>Environment: <code>config/environments/local.yaml</code>, <code>test.yaml</code>, <code>docker.yaml</code></li> <li>Deployment: <code>config/deployment/docker/docker-compose.yaml</code></li> </ul>"},{"location":"developer/architecture/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"developer/architecture/overview/#current-setup","title":"Current Setup","text":"<ul> <li>LLM: Foundation-Sec-8B (4.7GB, optimized for cybersecurity)</li> <li>Embeddings: nomic-embed-text-v1.5 (high-quality text embeddings)</li> <li>Vector Store: Qdrant (vector similarity search)</li> <li>Storage: Local file system (fast access, no network latency)</li> </ul>"},{"location":"developer/architecture/overview/#scalability-considerations","title":"Scalability Considerations","text":"<ul> <li>Horizontal Scaling: API layer can be scaled independently</li> <li>Vertical Scaling: Model performance scales with hardware</li> <li>Storage Scaling: Vector store can be moved to external database</li> </ul>"},{"location":"developer/architecture/overview/#development-architecture","title":"Development Architecture","text":""},{"location":"developer/architecture/overview/#code-organization","title":"Code Organization","text":"<pre><code>internal_assistant/\n\u251c\u2500\u2500 components/           # Core components (LLM, embeddings, etc.)\n\u251c\u2500\u2500 server/              # API endpoints and services\n\u251c\u2500\u2500 ui/                  # Gradio interface components\n\u251c\u2500\u2500 settings/            # Configuration management\n\u2514\u2500\u2500 utils/               # Utility functions\n</code></pre>"},{"location":"developer/architecture/overview/#testing-strategy","title":"Testing Strategy","text":"<ul> <li>Unit Tests: Component-level testing</li> <li>Integration Tests: API endpoint testing</li> <li>UI Tests: Interface functionality testing</li> <li>Performance Tests: Load and stress testing</li> </ul>"},{"location":"developer/architecture/overview/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Multi-agent Collaboration: Distributed threat analysis</li> <li>Enhanced Intelligence: Advanced pattern detection</li> <li>Real-time Updates: Live threat intelligence streaming</li> <li>Cloud Integration: Optional hybrid deployment</li> <li>Advanced Analytics: ML-based threat prediction</li> </ul>"},{"location":"developer/development/documentation-guidelines/","title":"Documentation Guidelines","text":"<p>Standards for creating and maintaining documentation.</p>"},{"location":"developer/development/documentation-guidelines/#writing-style","title":"Writing Style","text":"<ul> <li>Tone: Professional and clear</li> <li>Audience: Technical but accessible</li> <li>Format: Structured and scannable</li> <li>Length: Concise but complete</li> </ul>"},{"location":"developer/development/documentation-guidelines/#documentation-types","title":"Documentation Types","text":""},{"location":"developer/development/documentation-guidelines/#user-documentation-docsuser","title":"User Documentation (<code>docs/user/</code>)","text":"<p>Purpose: End-user guides for installation, configuration, and usage</p> <p>Required Sections: - Overview - Prerequisites - Step-by-step instructions - Troubleshooting - Examples</p> <p>Example: Installation Guide</p>"},{"location":"developer/development/documentation-guidelines/#api-documentation-docsapi","title":"API Documentation (<code>docs/api/</code>)","text":"<p>Purpose: Technical reference for developers</p> <p>Required Sections: - Endpoint descriptions - Request/response formats - Authentication - Error codes - Examples</p> <p>Example: API Reference</p>"},{"location":"developer/development/documentation-guidelines/#developer-documentation-docsdeveloper","title":"Developer Documentation (<code>docs/developer/</code>)","text":"<p>Purpose: Guides for contributors</p> <p>Required Sections: - Architecture overview - Setup instructions - Coding standards - Testing guidelines - Contribution process</p> <p>Example: Development Setup</p>"},{"location":"developer/development/documentation-guidelines/#file-naming","title":"File Naming","text":"<p>Use lowercase with hyphens: <code>installation-guide.md</code>, <code>api-reference.md</code></p> <p>Good: <code>installation-guide.md</code>, <code>quick-start.md</code></p> <p>Bad: <code>Installation Guide.md</code>, <code>quick_start.md</code>, <code>QuickStart.md</code></p>"},{"location":"developer/development/documentation-guidelines/#content-structure","title":"Content Structure","text":""},{"location":"developer/development/documentation-guidelines/#required-sections","title":"Required Sections","text":"<ol> <li>Title - Clear, descriptive heading</li> <li>Overview - Brief introduction (1-2 paragraphs)</li> <li>Main Content - Organized by topic with clear headings</li> <li>Related Links - Cross-references to related docs</li> </ol>"},{"location":"developer/development/documentation-guidelines/#optional-sections","title":"Optional Sections","text":"<ul> <li>Prerequisites - Required knowledge or setup</li> <li>Examples - Practical demonstrations</li> <li>Troubleshooting - Common issues and solutions</li> <li>Next Steps - Where to go after this guide</li> </ul>"},{"location":"developer/development/documentation-guidelines/#markdown-standards","title":"Markdown Standards","text":""},{"location":"developer/development/documentation-guidelines/#headers","title":"Headers","text":"<p>Use ATX-style headers (<code>#</code>, <code>##</code>, <code>###</code>):</p> <pre><code># Main Title\n## Section\n### Subsection\n</code></pre>"},{"location":"developer/development/documentation-guidelines/#code-blocks","title":"Code Blocks","text":"<p>Always specify language for syntax highlighting:</p> <pre><code>```python\ndef example():\n    return \"Hello\"\n```\n\n```bash\npoetry run make test\n```\n</code></pre>"},{"location":"developer/development/documentation-guidelines/#links","title":"Links","text":"<p>Use descriptive link text:</p> <p>Good: See the Installation Guide</p> <p>Bad: Click here</p>"},{"location":"developer/development/documentation-guidelines/#lists","title":"Lists","text":"<p>Use <code>-</code> for unordered lists, <code>1.</code> for ordered:</p> <pre><code>- Item one\n- Item two\n\n1. First step\n2. Second step\n</code></pre>"},{"location":"developer/development/documentation-guidelines/#mkdocs-integration","title":"MkDocs Integration","text":""},{"location":"developer/development/documentation-guidelines/#building-documentation","title":"Building Documentation","text":"<pre><code># Local development server\npoetry run mkdocs serve\n\n# Build static site\npoetry run mkdocs build\n</code></pre>"},{"location":"developer/development/documentation-guidelines/#navigation","title":"Navigation","text":"<p>Edit <code>mkdocs.yml</code> to add pages to navigation:</p> <pre><code>nav:\n  - Home: index.md\n  - User Guide:\n    - Installation: user/installation/installation.md\n    - Configuration: user/configuration/settings.md\n</code></pre>"},{"location":"developer/development/documentation-guidelines/#material-theme-features","title":"Material Theme Features","text":"<ul> <li> <p>Admonitions: Notes, warnings, tips   <pre><code>!!! note \"Title\"\n    Content here\n\n!!! warning\n    Important warning\n</code></pre></p> </li> <li> <p>Code Copy Buttons: Automatic for all code blocks</p> </li> <li>Search: Full-text search enabled by default</li> <li>Dark Mode: Theme switcher included</li> </ul>"},{"location":"developer/development/documentation-guidelines/#quality-checklist","title":"Quality Checklist","text":"<p>Before committing documentation:</p> <ul> <li>\u2705 All links work (no broken references)</li> <li>\u2705 Code examples tested and working</li> <li>\u2705 Spelling and grammar checked</li> <li>\u2705 Follows style guidelines</li> <li>\u2705 Builds without errors (<code>mkdocs build</code>)</li> <li>\u2705 Cross-references added where appropriate</li> <li>\u2705 Examples relevant to Internal Assistant</li> </ul>"},{"location":"developer/development/documentation-guidelines/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"developer/development/documentation-guidelines/#avoid","title":"Avoid","text":"<ul> <li>Broken links to non-existent files</li> <li>Outdated information</li> <li>Excessive verbosity</li> <li>Inconsistent formatting</li> <li>Missing code block language tags</li> <li>Relative links that don't work in MkDocs</li> </ul>"},{"location":"developer/development/documentation-guidelines/#best-practices","title":"Best Practices","text":"<ul> <li>Start with an outline</li> <li>Write for your audience</li> <li>Include practical examples</li> <li>Test all commands and code</li> <li>Link to related documentation</li> <li>Keep content current</li> </ul>"},{"location":"developer/development/documentation-guidelines/#review-process","title":"Review Process","text":"<ol> <li>Self-review: Check against guidelines</li> <li>Build test: Run <code>mkdocs build --strict</code></li> <li>Peer review: Get feedback from team</li> <li>Update: Address review comments</li> <li>Commit: Push to repository</li> </ol>"},{"location":"developer/development/documentation-guidelines/#tools","title":"Tools","text":""},{"location":"developer/development/documentation-guidelines/#recommended","title":"Recommended","text":"<ul> <li>Editor: VS Code with Markdown extensions</li> <li>Preview: Built-in markdown preview or MkDocs serve</li> <li>Spell check: VS Code extension or grammarly</li> <li>Link checker: <code>mkdocs build --strict</code></li> </ul>"},{"location":"developer/development/documentation-guidelines/#mkdocs-commands","title":"MkDocs Commands","text":"<pre><code># Start development server\npoetry run mkdocs serve\n\n# Build static site\npoetry run mkdocs build\n\n# Build with warnings as errors\npoetry run mkdocs build --strict\n\n# Deploy to GitHub Pages\npoetry run mkdocs gh-deploy\n</code></pre>"},{"location":"developer/development/documentation-guidelines/#examples","title":"Examples","text":""},{"location":"developer/development/documentation-guidelines/#good-documentation-example","title":"Good Documentation Example","text":"<pre><code># Installing Internal Assistant\n\nInternal Assistant requires Python 3.11.9 and Poetry 2.0+.\n\n## Prerequisites\n\n- Python 3.11.9 installed\n- Poetry installed\n- Git installed\n\n## Installation Steps\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/SecureYourGear/internal-assistant\n   cd internal-assistant\n   ```\n\n2. Install dependencies:\n   ```bash\n   poetry install --extras \"ui llms-ollama embeddings-huggingface vector-stores-qdrant\"\n   ```\n\n3. Run the application:\n   ```bash\n   poetry run make run\n   ```\n\n## Next Steps\n\n- [Configuration Guide](../configuration/settings.md)\n- [Quick Start](../usage/quickstart.md)\n</code></pre>"},{"location":"developer/development/documentation-guidelines/#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Ask in project discussions</li> <li>Issues: Report documentation problems via GitHub issues</li> <li>Improvements: Submit pull requests with fixes</li> </ul>"},{"location":"developer/development/documentation-guidelines/#references","title":"References","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>Markdown Guide</li> </ul>"},{"location":"developer/development/package-structure/","title":"Package Structure and Import Conventions","text":"<p>This guide explains the project structure and import patterns for Internal Assistant.</p> <p>Related Documentation: - System Architecture Overview - High-level architecture - Development Setup - Environment setup guide</p>"},{"location":"developer/development/package-structure/#package-structure","title":"Package Structure","text":""},{"location":"developer/development/package-structure/#key-directories","title":"Key Directories","text":"<ul> <li><code>internal_assistant/</code> - Main Python package (renamed from 'src')</li> <li><code>tests/</code> - Test suite</li> <li><code>config/</code> - Configuration files</li> <li><code>docs/</code> - Documentation</li> <li><code>tools/</code> - Development utilities</li> </ul>"},{"location":"developer/development/package-structure/#import-conventions","title":"Import Conventions","text":""},{"location":"developer/development/package-structure/#correct-import-patterns","title":"\u2705 Correct Import Patterns","text":"<p>Always use the full package name for imports:</p> <pre><code># \u2705 Correct - Use full package name\nfrom internal_assistant.components.vector_store.vector_store_component import VectorStoreComponent\nfrom internal_assistant.paths import local_data_path\nfrom internal_assistant.settings.settings import settings\nfrom internal_assistant.server.feeds.feeds_service import RSSFeedService\nfrom internal_assistant.ui.ui import InternalAssistantUI\n</code></pre>"},{"location":"developer/development/package-structure/#incorrect-import-patterns","title":"\u274c Incorrect Import Patterns","text":"<p>Never use relative imports or the old 'src' reference:</p> <pre><code># \u274c Wrong - Don't use relative imports\nfrom ..components.vector_store.vector_store_component import VectorStoreComponent\n\n# \u274c Wrong - Don't use old 'src' reference\nfrom src.components.vector_store.vector_store_component import VectorStoreComponent\n\n# \u274c Wrong - Don't use direct file imports\nfrom internal_assistant.components.vector_store import vector_store_component\n</code></pre>"},{"location":"developer/development/package-structure/#package-configuration","title":"Package Configuration","text":"<p>The package is configured in <code>pyproject.toml</code>:</p> <pre><code>[tool.poetry]\nname = \"internal-assistant\"\nversion = \"0.6.2\"\npackages = [\n    { include = \"internal_assistant\" }  # Maps internal_assistant/ directory\n]\n</code></pre>"},{"location":"developer/development/package-structure/#running-the-application","title":"Running the Application","text":""},{"location":"developer/development/package-structure/#development-mode","title":"Development Mode","text":"<pre><code># Using Makefile (recommended)\nmake dev\n\n# Direct command\npoetry run python -m uvicorn internal_assistant.main:app --reload --port 8001\n</code></pre>"},{"location":"developer/development/package-structure/#production-mode","title":"Production Mode","text":"<pre><code># Using Makefile\nmake production\n\n# Direct command\npoetry run python -m uvicorn internal_assistant.main:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"developer/development/package-structure/#module-execution","title":"Module Execution","text":"<pre><code># Run as module\npoetry run python -m internal_assistant\n\n# Run specific script\npoetry run python internal_assistant/main.py\n</code></pre>"},{"location":"developer/development/package-structure/#testing","title":"Testing","text":"<p>Tests should import using the full package name:</p> <pre><code># \u2705 Correct test imports\nfrom internal_assistant.server.feeds.feeds_service import RSSFeedService\nfrom internal_assistant.ui.ui import InternalAssistantUI\nfrom internal_assistant.di import create_application_injector\n</code></pre>"},{"location":"developer/development/package-structure/#tools-and-scripts","title":"Tools and Scripts","text":"<p>Tools in the <code>tools/</code> directory should import using the full package name:</p> <pre><code># \u2705 Correct tool imports\nfrom internal_assistant.paths import local_data_path\nfrom internal_assistant.settings.settings import settings\nfrom internal_assistant.utils.version_check import validate_dependency_versions\n</code></pre>"},{"location":"developer/development/package-structure/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"developer/development/package-structure/#issue-no-module-named-internal_assistant","title":"Issue: \"No module named 'internal_assistant'\"","text":"<p>Cause: Package not properly installed or PYTHONPATH not set Solution:  <pre><code># Reinstall package\npoetry install\n\n# Or run from project root\ncd /path/to/internal-assistant\npoetry run python -m internal_assistant\n</code></pre></p>"},{"location":"developer/development/package-structure/#issue-no-module-named-src","title":"Issue: \"No module named 'src'\"","text":"<p>Cause: Using old import patterns Solution: Update imports to use <code>internal_assistant</code> instead of <code>src</code></p>"},{"location":"developer/development/package-structure/#issue-import-errors-in-tests","title":"Issue: Import errors in tests","text":"<p>Cause: Test configuration not recognizing package structure Solution: Ensure tests use full package imports and run with <code>--import-mode=importlib</code></p>"},{"location":"developer/development/package-structure/#migration-guide","title":"Migration Guide","text":"<p>If you encounter old import patterns, here's how to fix them:</p>"},{"location":"developer/development/package-structure/#before-old-pattern","title":"Before (Old Pattern)","text":"<pre><code>from src.components.vector_store.vector_store_component import VectorStoreComponent\nfrom src.paths import local_data_path\nfrom src.settings.settings import settings\n</code></pre>"},{"location":"developer/development/package-structure/#after-new-pattern","title":"After (New Pattern)","text":"<pre><code>from internal_assistant.components.vector_store.vector_store_component import VectorStoreComponent\nfrom internal_assistant.paths import local_data_path\nfrom internal_assistant.settings.settings import settings\n</code></pre>"},{"location":"developer/development/package-structure/#file-path-references","title":"File Path References","text":"<p>When referencing file paths in code, use the correct directory structure:</p>"},{"location":"developer/development/package-structure/#before-old-pattern_1","title":"Before (Old Pattern)","text":"<pre><code>ui_file_path = os.path.join(os.path.dirname(__file__), '..', '..', 'src', 'ui', 'ui.py')\n</code></pre>"},{"location":"developer/development/package-structure/#after-new-pattern_1","title":"After (New Pattern)","text":"<pre><code>ui_file_path = os.path.join(os.path.dirname(__file__), '..', '..', 'internal_assistant', 'ui', 'ui.py')\n</code></pre>"},{"location":"developer/development/package-structure/#best-practices","title":"Best Practices","text":"<ol> <li>Always use full package imports - Never use relative imports</li> <li>Use the Makefile - It contains the correct commands for running the application</li> <li>Test imports work - Run <code>poetry run make test</code> to verify imports are correct</li> <li>Update documentation - When changing import patterns, update this guide</li> <li>Use IDE features - Configure your IDE to recognize the <code>internal_assistant</code> package</li> </ol>"},{"location":"developer/development/package-structure/#verification-commands","title":"Verification Commands","text":"<p>To verify the package structure is correct:</p> <pre><code># Check if package can be imported\npoetry run python -c \"import internal_assistant; print('\u2705 Package imports correctly')\"\n\n# Run tests to verify all imports work\npoetry run make test\n\n# Check package structure\npoetry run python -c \"from internal_assistant import main; print('\u2705 Main module accessible')\"\n</code></pre>"},{"location":"developer/development/package-structure/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter import issues:</p> <ol> <li>Check pyproject.toml - Ensure package configuration is correct</li> <li>Verify directory structure - Ensure <code>internal_assistant/</code> directory exists</li> <li>Reinstall package - Run <code>poetry install</code> to refresh package installation</li> <li>Check PYTHONPATH - Ensure you're running from the project root</li> <li>Update imports - Replace any remaining <code>src</code> references with <code>internal_assistant</code></li> </ol>"},{"location":"developer/development/package-structure/#related-documentation","title":"Related Documentation","text":"<ul> <li>Development Setup</li> <li>Testing Guide</li> <li>Configuration Guide</li> <li>API Documentation</li> </ul> <p>Note: This document should be updated whenever the package structure changes to prevent future import issues.</p>"},{"location":"developer/development/setup/","title":"Development Setup Guide","text":"<p>This guide covers setting up the Internal Assistant project for development.</p>"},{"location":"developer/development/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11.9 (required for all dependencies)</li> <li>Poetry 2.0+ (dependency management)</li> <li>Git (version control)</li> </ul>"},{"location":"developer/development/setup/#quick-start","title":"Quick Start","text":""},{"location":"developer/development/setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone &lt;repository-url&gt;\ncd internal-assistant\n</code></pre>"},{"location":"developer/development/setup/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code># Install Poetry if not already installed\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Install project dependencies\npoetry install\n</code></pre>"},{"location":"developer/development/setup/#3-setup-environment","title":"3. Setup Environment","text":"<pre><code># Copy environment template (if needed)\ncp .env.example .env\n\n# Install pre-commit hooks\npoetry run pre-commit install\n</code></pre>"},{"location":"developer/development/setup/#4-run-the-application","title":"4. Run the Application","text":"<pre><code># Start the application\npoetry run make run\n\n# Or development mode with auto-reload\npoetry run make dev\n</code></pre>"},{"location":"developer/development/setup/#modern-poetry-workflow","title":"Modern Poetry Workflow","text":"<p>This project uses Poetry 2.0+ with modern execution approach:</p>"},{"location":"developer/development/setup/#recommended-commands","title":"Recommended Commands","text":"<pre><code># Start application (recommended)\npoetry run make run\n\n# Development mode\npoetry run make dev\n\n# Run tests\npoetry run make test\n\n# Interactive development\npoetry env activate\nmake run\ndeactivate\n</code></pre>"},{"location":"developer/development/setup/#deprecated-commands","title":"Deprecated Commands","text":"<ul> <li>\u274c <code>poetry shell</code> (removed in Poetry 2.0+)</li> <li>\u274c <code>make run</code> without <code>poetry run</code> (won't work)</li> </ul>"},{"location":"developer/development/setup/#project-structure","title":"Project Structure","text":"<pre><code>internal-assistant/\n\u251c\u2500\u2500 internal_assistant/     # Main application code\n\u251c\u2500\u2500 config/                # Configuration files\n\u2502   \u251c\u2500\u2500 app/              # Core settings\n\u2502   \u251c\u2500\u2500 model-configs/    # Model configurations\n\u2502   \u251c\u2500\u2500 environments/     # Environment-specific configs\n\u2502   \u2514\u2500\u2500 deployment/       # Deployment configs\n\u251c\u2500\u2500 data/                 # Application data\n\u2502   \u251c\u2500\u2500 runtime/          # Logs, cache, temp\n\u2502   \u251c\u2500\u2500 persistent/       # Storage, documents\n\u2502   \u2514\u2500\u2500 models/           # Model files (future)\n\u251c\u2500\u2500 tools/                # Development tools\n\u2502   \u251c\u2500\u2500 maintenance/      # Maintenance scripts\n\u2502   \u2514\u2500\u2500 system/           # System utilities\n\u251c\u2500\u2500 tests/                # Test suite\n\u2514\u2500\u2500 docs/                 # Documentation\n</code></pre>"},{"location":"developer/development/setup/#configuration","title":"Configuration","text":""},{"location":"developer/development/setup/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>PGPT_PROFILES</code>: Comma-separated list of profiles to load</li> <li><code>PGPT_SETTINGS_FOLDER</code>: Override config directory path</li> </ul>"},{"location":"developer/development/setup/#local-development","title":"Local Development","text":"<pre><code>PGPT_PROFILES=local poetry run make run\n</code></pre>"},{"location":"developer/development/setup/#testing","title":"Testing","text":""},{"location":"developer/development/setup/#run-all-tests","title":"Run All Tests","text":"<pre><code>poetry run make test\n</code></pre>"},{"location":"developer/development/setup/#run-specific-tests","title":"Run Specific Tests","text":"<pre><code># Run server tests\npoetry run pytest tests/server/\n\n# Run UI tests\npoetry run pytest tests/ui/\n\n# Run with coverage\npoetry run pytest --cov=internal_assistant\n</code></pre>"},{"location":"developer/development/setup/#development-tools","title":"Development Tools","text":""},{"location":"developer/development/setup/#code-quality","title":"Code Quality","text":"<pre><code># Format code\npoetry run black .\n\n# Lint code\npoetry run ruff check .\n\n# Type checking\npoetry run mypy internal_assistant/\n</code></pre>"},{"location":"developer/development/setup/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>The project uses pre-commit hooks for code quality: - Black: Code formatting - Ruff: Linting and import sorting - MyPy: Type checking</p>"},{"location":"developer/development/setup/#utility-scripts","title":"Utility Scripts","text":"<pre><code># Check compatibility\npoetry run make compatibility-check\n\n# Clean up logs\npoetry run make log-cleanup\n\n# Analyze models\npoetry run make analyze-models\n</code></pre>"},{"location":"developer/development/setup/#documentation-development","title":"Documentation Development","text":""},{"location":"developer/development/setup/#mkdocs-local-development","title":"MkDocs Local Development","text":"<p>The project uses MkDocs with Material theme for all documentation.</p>"},{"location":"developer/development/setup/#quick-start_1","title":"Quick Start","text":"<pre><code># Install docs dependencies (if not already installed)\npoetry install --with docs\n\n# Start live development server\npoetry run mkdocs serve\n# \u2192 Serves at http://localhost:8000\n# \u2192 Auto-reloads on file changes\n</code></pre>"},{"location":"developer/development/setup/#documentation-commands","title":"Documentation Commands","text":"<pre><code># Development server (recommended for editing)\npoetry run mkdocs serve\n# \u2192 Live preview with hot reload\n# \u2192 Automatically rebuilds on save\n# \u2192 Shows warnings and errors\n\n# Build static site\npoetry run mkdocs build\n# \u2192 Generates site/ folder (3.7MB)\n# \u2192 21 HTML pages from markdown source\n# \u2192 Full-text search index\n# \u2192 Optimized assets\n\n# Serve built site locally\npoetry run mkdocs serve --dev-addr=8001\n# \u2192 Test the built version\n</code></pre>"},{"location":"developer/development/setup/#documentation-structure","title":"Documentation Structure","text":"<pre><code>docs/                    # Source files (markdown)\n\u251c\u2500\u2500 user/               # User documentation\n\u251c\u2500\u2500 developer/          # Developer documentation  \n\u251c\u2500\u2500 api/                # API reference\n\u2514\u2500\u2500 assets/             # Images, logos\n\nsite/                   # Built website (git-ignored)\n\u251c\u2500\u2500 search/             # Search functionality (3MB index)\n\u251c\u2500\u2500 assets/             # Optimized CSS/JS/images  \n\u2514\u2500\u2500 *.html              # Generated HTML pages\n</code></pre>"},{"location":"developer/development/setup/#writing-documentation","title":"Writing Documentation","text":"<pre><code># 1. Edit markdown files in docs/\nvim docs/user/new-guide.md\n\n# 2. Preview changes live\npoetry run mkdocs serve\n\n# 3. Add to navigation in mkdocs.yml\n# 4. Build to verify\npoetry run mkdocs build\n</code></pre>"},{"location":"developer/development/setup/#documentation-features-available","title":"Documentation Features Available","text":"<ul> <li>Material Design theme with dark/light mode</li> <li>Full-text search with 25 language support</li> <li>Code highlighting with copy buttons</li> <li>Mermaid diagrams support</li> <li>Admonitions (notes, warnings, tips)</li> <li>Tabs and collapsible sections</li> <li>Auto-generated navigation</li> </ul>"},{"location":"developer/development/setup/#common-documentation-tasks","title":"Common Documentation Tasks","text":"<pre><code># Check for broken links\npoetry run mkdocs build 2&gt;&amp;1 | grep -i warning\n\n# Verify all pages build correctly\npoetry run mkdocs build --strict\n\n# Check documentation size\ndu -sh site/\n\n# Verify search index\nls -la site/search/search_index.json\n</code></pre>"},{"location":"developer/development/setup/#important-notes","title":"Important Notes","text":"<ul> <li>site/ folder is git-ignored - don't commit build output</li> <li>Build time: ~4-5 seconds for complete site</li> <li>Search index: ~3MB - enables fast full-text search</li> <li>21 markdown files \u2192 21 HTML pages - perfect 1:1 conversion</li> </ul>"},{"location":"developer/development/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/development/setup/#common-issues","title":"Common Issues","text":""},{"location":"developer/development/setup/#poetry-environment-issues","title":"Poetry Environment Issues","text":"<pre><code># Recreate virtual environment\npoetry env remove python\npoetry install\n</code></pre>"},{"location":"developer/development/setup/#dependency-conflicts","title":"Dependency Conflicts","text":"<pre><code># Update dependencies\npoetry update\n\n# Check compatibility\npoetry run make compatibility-check\n</code></pre>"},{"location":"developer/development/setup/#qdrant-lock-issues","title":"Qdrant Lock Issues","text":"<pre><code># Clean up Qdrant locks\npoetry run python tools/maintenance/cleanup_qdrant.py\n</code></pre>"},{"location":"developer/development/setup/#getting-help","title":"Getting Help","text":"<ol> <li>Check the troubleshooting guide</li> <li>Review configuration documentation</li> <li>See the architecture overview</li> </ol>"},{"location":"developer/development/setup/#next-steps","title":"Next Steps","text":"<ul> <li>Review architecture documentation</li> <li>Explore package structure</li> <li>Read documentation guidelines</li> </ul>"},{"location":"user/configuration/llms/","title":"LLM Configuration","text":"<p>Configure the Large Language Model (LLM) provider for Internal Assistant.</p>"},{"location":"user/configuration/llms/#supported-providers","title":"Supported Providers","text":"<ul> <li>Local - Ollama or LlamaCPP for local execution</li> <li>OpenAI - OpenAI cloud models</li> <li>Azure OpenAI - Microsoft Azure OpenAI service</li> <li>Google Gemini - Google's Gemini models</li> <li>AWS Sagemaker - Amazon Sagemaker endpoints</li> <li>Mock - Testing mode with simulated responses</li> </ul>"},{"location":"user/configuration/llms/#local-llm-configuration","title":"Local LLM Configuration","text":"<p>Run models locally for complete privacy and control.</p>"},{"location":"user/configuration/llms/#basic-configuration","title":"Basic Configuration","text":"<p>Edit <code>config/model-configs/foundation-sec.yaml</code> or <code>config/model-configs/ollama.yaml</code>:</p> <pre><code>llm:\n  mode: local\n  max_new_tokens: 1000\n  temperature: 0.1\n  context_window: 8192\n</code></pre>"},{"location":"user/configuration/llms/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>mode</code> LLM provider <code>local</code> <code>max_new_tokens</code> Maximum response length <code>1000</code> <code>temperature</code> Creativity (0.0-1.0) <code>0.1</code> <code>context_window</code> Input context size <code>8192</code> <code>top_p</code> Nucleus sampling <code>0.9</code> <code>top_k</code> Top-k sampling <code>40</code>"},{"location":"user/configuration/llms/#gpu-configuration","title":"GPU Configuration","text":"<p>For GPU users: <pre><code>llm:\n  gpu_layers: 35    # Offload layers to GPU\n  n_ctx: 8192       # Context window\n</code></pre></p> <p>For CPU users: <pre><code>llm:\n  gpu_layers: 0     # Use CPU only\n  n_ctx: 4096       # Smaller context for CPU\n</code></pre></p>"},{"location":"user/configuration/llms/#supported-models","title":"Supported Models","text":"<ul> <li>Foundation-Sec-8B - Cybersecurity-focused (recommended)</li> <li>Llama \u2154 - General purpose</li> <li>Mistral - High performance</li> <li>Custom GGUF models - Any compatible model</li> </ul>"},{"location":"user/configuration/llms/#running","title":"Running","text":"<pre><code>PGPT_PROFILES=ollama make run\n</code></pre>"},{"location":"user/configuration/llms/#openai-configuration","title":"OpenAI Configuration","text":"<p>Use OpenAI's cloud models. Note: data will be sent to OpenAI.</p> <p>Edit <code>config/model-configs/openai.yaml</code>:</p> <pre><code>llm:\n  mode: openai\n\nopenai:\n  api_key: ${OPENAI_API_KEY}\n  model: gpt-4\n  max_tokens: 1000\n</code></pre> <p>Set API key: <pre><code>export OPENAI_API_KEY=\"your-key\"\nPGPT_PROFILES=openai make run\n</code></pre></p>"},{"location":"user/configuration/llms/#openai-compatible-apis","title":"OpenAI-Compatible APIs","text":"<p>Use any service that implements the OpenAI API format (LocalAI, vLLM, etc.):</p> <pre><code>llm:\n  mode: openailike\n\nopenai:\n  api_base: http://localhost:8000/v1\n  model: your-model-name\n</code></pre>"},{"location":"user/configuration/llms/#mock-mode-testing","title":"Mock Mode (Testing)","text":"<p>Use simulated responses for testing without models:</p> <pre><code>llm:\n  mode: mock\n</code></pre> <p>Use for: Testing, CI/CD, development without GPU</p> <pre><code>PGPT_PROFILES=mock make run\n</code></pre>"},{"location":"user/configuration/llms/#performance-tuning","title":"Performance Tuning","text":""},{"location":"user/configuration/llms/#response-quality","title":"Response Quality","text":"<p>Factual responses (recommended for cybersecurity): <pre><code>llm:\n  temperature: 0.1\n  top_p: 0.95\n  top_k: 40\n</code></pre></p> <p>Creative responses: <pre><code>llm:\n  temperature: 0.7\n  top_p: 0.9\n  top_k: 50\n</code></pre></p>"},{"location":"user/configuration/llms/#memory-optimization","title":"Memory Optimization","text":"<p>Reduce memory usage: <pre><code>llm:\n  max_new_tokens: 512    # Smaller responses\n  n_ctx: 4096            # Smaller context\n  gpu_layers: 20         # Fewer GPU layers\n</code></pre></p>"},{"location":"user/configuration/llms/#troubleshooting","title":"Troubleshooting","text":"<p>See the Troubleshooting Guide for common LLM issues.</p>"},{"location":"user/configuration/settings/","title":"Settings and Configuration","text":"<p>The configuration of your Internal Assistant server is done thanks to <code>settings</code> files (more precisely <code>settings.yaml</code>). These text files are written using the YAML syntax.</p> <p>While Internal Assistant is distributing safe and universal configuration files, you might want to quickly customize your Internal Assistant, and this can be done using the <code>settings</code> files.</p> <p>This project is defining the concept of profiles (or configuration profiles). This mechanism, using your environment variables, is giving you the ability to easily switch between configuration you've made.</p> <p>A typical use case of profile is to easily switch between LLM and embeddings. To be a bit more precise, you can change the language (to French, Spanish, Italian, English, etc) by simply changing the profile you've selected; no code changes required!</p> <p>Internal Assistant is configured through profiles that are defined using yaml files, and selected through env variables. The full list of properties configurable can be found in <code>settings.yaml</code>.</p>"},{"location":"user/configuration/settings/#how-to-know-which-profiles-exist","title":"How to know which profiles exist","text":"<p>Profiles are stored in multiple locations: - Environment profiles: <code>config/environments/{profile}.yaml</code> (e.g., <code>local.yaml</code>, <code>test.yaml</code>, <code>docker.yaml</code>) - Model profiles: <code>config/model-configs/{profile}.yaml</code> (e.g., <code>foundation-sec.yaml</code>, <code>ollama.yaml</code>) - Legacy profiles: <code>config/settings-{profile}.yaml</code> (for backward compatibility)</p> <p>To see available profiles: <pre><code>ls config/environments/\nls config/model-configs/\n</code></pre></p>"},{"location":"user/configuration/settings/#how-to-use-an-existing-profiles","title":"How to use an existing profiles","text":"<p>Please note that the syntax to set the value of an environment variables depends on your OS. You have to set environment variable <code>PGPT_PROFILES</code> to the name of the profile you want to use.</p> <p>For example, on linux and macOS, this gives: <pre><code>export PGPT_PROFILES=my_profile_name_here\n</code></pre></p> <p>Windows Command Prompt (cmd) has a different syntax: <pre><code>set PGPT_PROFILES=my_profile_name_here\n</code></pre></p> <p>Windows Powershell has a different syntax: <pre><code>$env:PGPT_PROFILES=\"my_profile_name_here\"\n</code></pre> If the above is not working, you might want to try other ways to set an env variable in your window's terminal.</p> <p>Once you've set this environment variable to the desired profile, you can simply launch your Internal Assistant, and it will run using your profile on top of the default configuration.</p>"},{"location":"user/configuration/settings/#reference","title":"Reference","text":"<p>Additional details on the profiles are described in this section</p>"},{"location":"user/configuration/settings/#environment-variable-pgpt_settings_folder","title":"Environment variable <code>PGPT_SETTINGS_FOLDER</code>","text":"<p>The location of the settings folder. Defaults to <code>config/</code>. Should contain the default <code>settings.yaml</code> and profile directories.</p>"},{"location":"user/configuration/settings/#environment-variable-pgpt_profiles","title":"Environment variable <code>PGPT_PROFILES</code>","text":"<p>By default, the configuration in <code>config/settings.yaml</code> is loaded. Using this env var you can load additional profiles; format is a comma separated list of profile names.</p> <p>The system searches for profiles in this order: 1. <code>config/environments/{profile}.yaml</code> 2. <code>config/model-configs/{profile}.yaml</code> 3. <code>config/settings-{profile}.yaml</code> (legacy)</p> <p>Profile contents are merged on top of the base settings, with later profiles overriding earlier ones.</p> <p>Examples: - <code>PGPT_PROFILES=local</code> loads <code>config/environments/local.yaml</code> - <code>PGPT_PROFILES=test</code> loads <code>config/environments/test.yaml</code> - <code>PGPT_PROFILES=local,docker</code> loads both profiles, with docker settings taking precedence</p>"},{"location":"user/configuration/settings/#environment-variables-expansion","title":"Environment variables expansion","text":"<p>Configuration files can contain environment variables, they will be expanded at runtime.</p> <p>Expansion must follow the pattern <code>${VARIABLE_NAME:default_value}</code>.</p> <p>For example, the following configuration will use the value of the <code>PORT</code> environment variable or <code>8001</code> if it's not set. Missing variables with no default will produce an error.</p> <pre><code>server:\n  port: ${PORT:8001}\n</code></pre>"},{"location":"user/installation/concepts/","title":"Core Concepts","text":"<p>Internal Assistant is a cybersecurity intelligence platform that provides a private, secure AI system for threat analysis and security research. It wraps RAG (Retrieval Augmented Generation) capabilities in a complete API framework.</p> <p>The platform uses FastAPI for the API layer and LlamaIndex as the core RAG framework. It supports multiple LLM providers, embedding models, and vector stores that can be configured without code changes.</p>"},{"location":"user/installation/concepts/#configurable-components","title":"Configurable Components","text":"<p>Internal Assistant has three main configurable components:</p> <ul> <li>LLM: The large language model used for inference. Can be local (Ollama, LlamaCPP) or cloud-based (OpenAI, Azure, Gemini, Sagemaker).</li> <li>Embeddings: The model used to encode documents and queries. Can be local (HuggingFace, Ollama) or cloud-based.</li> <li>Vector Store: The database used to index and retrieve documents (Qdrant, Milvus, Chroma, PostgreSQL, ClickHouse).</li> </ul> <p>An optional Gradio web UI is available for easy interaction with the platform through a web interface.</p>"},{"location":"user/installation/concepts/#dependencies-and-installation","title":"Dependencies and Installation","text":"<p>Internal Assistant uses Poetry for dependency management. Install only the components you need using extras:</p> <pre><code>poetry install --extras \"ui vector-stores-qdrant llms-ollama embeddings-huggingface\"\n</code></pre> <p>See the Installation Guide for recommended setup combinations.</p>"},{"location":"user/installation/concepts/#configuration","title":"Configuration","text":"<p>Configuration is managed through YAML files in the <code>config/</code> directory. The system supports multiple profiles that can be loaded using the <code>PGPT_PROFILES</code> environment variable.</p> <p>Example: <pre><code>PGPT_PROFILES=ollama make run\n</code></pre></p> <p>This loads: 1. <code>config/settings.yaml</code> - Base configuration (always loaded) 2. <code>config/model-configs/ollama.yaml</code> - Ollama-specific settings</p> <p>Profile-specific settings override base settings.</p>"},{"location":"user/installation/concepts/#fully-local-setup","title":"Fully Local Setup","text":"<p>For a completely local, privacy-focused deployment:</p>"},{"location":"user/installation/concepts/#llm-options","title":"LLM Options","text":"<p>Ollama (Recommended) - Simplifies local model management - Handles GPU acceleration automatically - Install: <code>poetry install --extras \"llms-ollama\"</code></p> <p>LlamaCPP - Direct model file execution - Best for macOS with Metal GPU - May require GPU-specific configuration on Linux/Windows - Install: <code>poetry install --extras \"llms-llama-cpp\"</code></p> <p>For LlamaCPP, download models using: <pre><code>poetry run python tools/system/manage_compatibility.py --check\n</code></pre></p>"},{"location":"user/installation/concepts/#embeddings-options","title":"Embeddings Options","text":"<p>HuggingFace (Recommended for local) - Local embeddings with no external API calls - Install: <code>poetry install --extras \"embeddings-huggingface\"</code></p> <p>Ollama - If using Ollama for LLM, can also handle embeddings - Install: <code>poetry install --extras \"embeddings-ollama\"</code></p>"},{"location":"user/installation/concepts/#vector-stores","title":"Vector Stores","text":"<p>All supported vector stores (Qdrant, Milvus, Chroma, PostgreSQL) can run locally. Qdrant is the recommended default.</p>"},{"location":"user/installation/installation/","title":"Installation Guide","text":"<p>Before starting, review the Core Concepts to understand Internal Assistant's architecture and components.</p>"},{"location":"user/installation/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"user/installation/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/SecureYourGear/internal-assistant\ncd internal-assistant\n</code></pre>"},{"location":"user/installation/installation/#2-install-python-3119","title":"2. Install Python 3.11.9","text":"<p>Internal Assistant requires Python 3.11.9 (exact version). Install using a version manager:</p> <p>macOS/Linux (using pyenv): <pre><code>pyenv install 3.11.9\npyenv local 3.11.9\n</code></pre></p> <p>Windows (using pyenv-win): <pre><code>pyenv install 3.11.9\npyenv local 3.11.9\n</code></pre></p>"},{"location":"user/installation/installation/#3-install-poetry","title":"3. Install Poetry","text":"<p>Install Poetry for dependency management:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Required Version: Poetry 2.0+ (older versions are not compatible)</p> <p>To upgrade: <pre><code>poetry self update\n</code></pre></p>"},{"location":"user/installation/installation/#4-install-make-optional-but-recommended","title":"4. Install Make (Optional but Recommended)","text":"<p>macOS: <pre><code>brew install make\n</code></pre></p> <p>Windows: <pre><code>choco install make\n</code></pre></p>"},{"location":"user/installation/installation/#installation-options","title":"Installation Options","text":"<p>Install only the components you need by selecting extras during installation:</p> <pre><code>poetry install --extras \"&lt;extra1&gt; &lt;extra2&gt;...\"\n</code></pre>"},{"location":"user/installation/installation/#available-components","title":"Available Components","text":"<p>Choose one option per category:</p>"},{"location":"user/installation/installation/#llm","title":"LLM","text":"Option Description Extra ollama Adds support for Ollama LLM, requires Ollama running locally llms-ollama llama-cpp Adds support for local LLM using LlamaCPP llms-llama-cpp sagemaker Adds support for Amazon Sagemaker LLM, requires Sagemaker endpoints llms-sagemaker openai Adds support for OpenAI LLM, requires OpenAI API key llms-openai openailike Adds support for 3<sup>rd</sup> party LLM providers compatible with OpenAI's API llms-openai-like azopenai Adds support for Azure OpenAI LLM, requires Azure endpoints llms-azopenai gemini Adds support for Gemini LLM, requires Gemini API key llms-gemini"},{"location":"user/installation/installation/#embeddings","title":"Embeddings","text":"Option Description Extra ollama Adds support for Ollama Embeddings, requires Ollama running locally embeddings-ollama huggingface Adds support for local Embeddings using HuggingFace embeddings-huggingface openai Adds support for OpenAI Embeddings, requires OpenAI API key embeddings-openai sagemaker Adds support for Amazon Sagemaker Embeddings, requires Sagemaker endpoints embeddings-sagemaker azopenai Adds support for Azure OpenAI Embeddings, requires Azure endpoints embeddings-azopenai gemini Adds support for Gemini Embeddings, requires Gemini API key embeddings-gemini"},{"location":"user/installation/installation/#vector-stores","title":"Vector Stores","text":"Option Description Extra qdrant Adds support for Qdrant vector store vector-stores-qdrant milvus Adds support for Milvus vector store vector-stores-milvus chroma Adds support for Chroma DB vector store vector-stores-chroma postgres Adds support for Postgres vector store vector-stores-postgres clickhouse Adds support for Clickhouse vector store vector-stores-clickhouse"},{"location":"user/installation/installation/#ui","title":"UI","text":"Option Description Extra Gradio Web interface for the platform ui"},{"location":"user/installation/installation/#recommended-setups","title":"Recommended Setups","text":"<p>Below are tested setup combinations. Choose based on your requirements.</p> <p>Windows Users Note: Set environment variables using PowerShell or CMD syntax:</p> <pre><code># PowerShell\n$env:PGPT_PROFILES=\"ollama\"\nmake run\n</code></pre> <pre><code># CMD\nset PGPT_PROFILES=ollama\nmake run\n</code></pre>"},{"location":"user/installation/installation/#local-setup-with-ollama-recommended","title":"Local Setup with Ollama (Recommended)","text":"<p>This is the recommended setup for fully local operation with Foundation-Sec-8B cybersecurity model.</p> <p>1. Install Ollama</p> <p>Visit ollama.ai and install Ollama for your platform.</p> <p>2. Start Ollama Service</p> <pre><code>ollama serve\n</code></pre> <p>3. Pull Models</p> <p>Internal Assistant automatically pulls models when needed, or pull manually:</p> <pre><code>ollama pull foundation-sec-q4km:latest    # ~4.7GB cybersecurity LLM\nollama pull nomic-embed-text             # ~275MB embeddings\n</code></pre> <p>4. Install Internal Assistant</p> <pre><code>poetry install --extras \"ui llms-ollama embeddings-huggingface vector-stores-qdrant\"\n</code></pre> <p>5. Run the Application</p> <pre><code>PGPT_PROFILES=ollama make run\n</code></pre> <p>The UI will be available at http://localhost:8001</p> <p>Configuration is in <code>config/model-configs/foundation-sec.yaml</code> and <code>config/model-configs/ollama.yaml</code>.</p>"},{"location":"user/installation/installation/#cloud-based-setups","title":"Cloud-Based Setups","text":"<p>For cloud-based deployments, Internal Assistant supports various providers. Note that data will be sent to external services.</p> <p>AWS Sagemaker <pre><code>poetry install --extras \"ui llms-sagemaker embeddings-sagemaker vector-stores-qdrant\"\n# Configure endpoints in config/model-configs/sagemaker.yaml\nPGPT_PROFILES=sagemaker make run\n</code></pre></p> <p>OpenAI <pre><code>poetry install --extras \"ui llms-openai embeddings-openai vector-stores-qdrant\"\n# Set OPENAI_API_KEY environment variable\nPGPT_PROFILES=openai make run\n</code></pre></p> <p>Azure OpenAI <pre><code>poetry install --extras \"ui llms-azopenai embeddings-azopenai vector-stores-qdrant\"\n# Configure endpoints in config/model-configs/azure-openai.yaml\nPGPT_PROFILES=azopenai make run\n</code></pre></p> <p>Google Gemini <pre><code>poetry install --extras \"ui llms-gemini embeddings-gemini vector-stores-qdrant\"\n# Configure in config/model-configs/gemini.yaml\nPGPT_PROFILES=gemini make run\n</code></pre></p>"},{"location":"user/installation/installation/#alternative-local-setup-with-llamacpp","title":"Alternative Local Setup with LlamaCPP","text":"<p>For advanced users who prefer direct model file execution without Ollama:</p> <pre><code>poetry install --extras \"ui llms-llama-cpp embeddings-huggingface vector-stores-qdrant\"\n</code></pre> <p>Download Models <pre><code>poetry run python tools/system/manage_compatibility.py --check\n</code></pre></p> <p>Run <pre><code>PGPT_PROFILES=local make run\n</code></pre></p> <p>Requirements: - C++ compiler (gcc, clang, or MSVC) - See Troubleshooting for compiler setup - macOS users: Metal GPU support requires specific compilation flags - Consult llama-cpp-python documentation for platform-specific instructions</p>"},{"location":"user/installation/troubleshooting/","title":"Troubleshooting","text":""},{"location":"user/installation/troubleshooting/#downloading-gated-and-private-models","title":"Downloading Gated and Private Models","text":"<p>Many models are gated or private, requiring special access to use them. Follow these steps to gain access and set up your environment for using these models.</p>"},{"location":"user/installation/troubleshooting/#accessing-gated-models","title":"Accessing Gated Models","text":"<ol> <li>Request Access:    Follow the instructions provided here to request access to the gated model.</li> <li>Generate a Token:    Once you have access, generate a token by following the instructions here.</li> <li>Set the Token:    Add the generated token to your <code>settings.yaml</code> file:    <pre><code>huggingface:\n  access_token: &lt;your-token&gt;\n</code></pre>    Alternatively, set the <code>HF_TOKEN</code> environment variable:    <pre><code>export HF_TOKEN=&lt;your-token&gt;\n</code></pre></li> </ol>"},{"location":"user/installation/troubleshooting/#tokenizer-setup","title":"Tokenizer Setup","text":"<p>Internal Assistant uses the <code>AutoTokenizer</code> library to tokenize input text accurately. It connects to HuggingFace's API to download the appropriate tokenizer for the specified model.</p>"},{"location":"user/installation/troubleshooting/#configuring-the-tokenizer","title":"Configuring the Tokenizer","text":"<ol> <li>Specify the Model:    In your <code>settings.yaml</code> file, specify the model you want to use:    <pre><code>llm:\n  tokenizer: local/Foundation-Sec-8B\n</code></pre></li> <li>Set Access Token for Gated Models:    If you are using a gated model, ensure the <code>access_token</code> is set as mentioned in the previous section.</li> </ol> <p>This configuration ensures that Internal Assistant can download and use the correct tokenizer for the model you are working with.</p>"},{"location":"user/installation/troubleshooting/#embedding-dimensions-mismatch","title":"Embedding dimensions mismatch","text":"<p>If you encounter an error message like <code>Embedding dimensions mismatch</code>, it is likely due to the embedding model and current vector dimension mismatch. To resolve this issue, ensure that the model and the input data have the same vector dimensions.</p> <p>By default, Internal Assistant uses <code>nomic-embed-text</code> embeddings, which have a vector dimension of 768. If you are using a different embedding model, ensure that the vector dimensions match the model's output.</p> <p>Version Compatibility</p> <p>In versions below to 0.6.0, the default embedding model was <code>BAAI/bge-small-en-v1.5</code> in <code>huggingface</code> setup. If you plan to reuse the old generated embeddings, you need to update the <code>settings.yaml</code> file to use the correct embedding model: <pre><code>huggingface:\n  embedding_hf_model_name: BAAI/bge-small-en-v1.5\nembedding:\n  embed_dim: 384\n</code></pre></p>"},{"location":"user/installation/troubleshooting/#building-llama-cpp-with-nvidia-gpu-support","title":"Building Llama-cpp with NVIDIA GPU support","text":""},{"location":"user/installation/troubleshooting/#out-of-memory-error","title":"Out-of-memory error","text":"<p>If you encounter an out-of-memory error while running <code>llama-cpp</code> with CUDA, you can try the following steps to resolve the issue: 1. Set the next environment: <pre><code>TOKENIZERS_PARALLELISM=true\n</code></pre> 2. Run Internal Assistant: <pre><code>poetry run python -m internal_assistant\n</code></pre></p> <p>Credits</p> <p>Give thanks to MarioRossiGithub for providing the following solution.</p>"},{"location":"user/installation/troubleshooting/#troubleshooting-c-compiler","title":"Troubleshooting C++ Compiler","text":"<p>If you encounter issues with C++ compilation when installing llama-cpp-python, ensure you have a valid C++ compiler installed:</p>"},{"location":"user/installation/troubleshooting/#linux","title":"Linux","text":"<pre><code>sudo apt-get install build-essential\n</code></pre>"},{"location":"user/installation/troubleshooting/#macos","title":"macOS","text":"<pre><code>xcode-select --install\n</code></pre>"},{"location":"user/installation/troubleshooting/#windows","title":"Windows","text":"<p>Install Visual Studio Build Tools or use WSL (Windows Subsystem for Linux).</p>"},{"location":"user/installation/troubleshooting/#llm-issues-llm-issues","title":"LLM Issues {#llm-issues}","text":""},{"location":"user/installation/troubleshooting/#common-llm-problems","title":"Common LLM Problems","text":"<p>Out of Memory Errors - Reduce <code>gpu_layers</code> or <code>n_ctx</code> in settings - Use a smaller model - Close other applications - Use CPU-only mode</p> <p>Slow Response Times - Increase <code>gpu_layers</code> for GPU users - Use a GPU with more memory - Reduce <code>max_new_tokens</code> - Consider cloud-based models</p> <p>Model Loading Errors - Verify model file integrity - Check model format compatibility - Ensure sufficient disk space - Update llama-cpp-python</p> <p>API Connection Issues - Verify API key and base URL - Check network connectivity - Ensure service is running - Verify rate limits</p>"},{"location":"user/installation/troubleshooting/#vector-database-issues-vector-database-issues","title":"Vector Database Issues {#vector-database-issues}","text":""},{"location":"user/installation/troubleshooting/#common-vector-database-problems","title":"Common Vector Database Problems","text":"<p>Qdrant Connection Errors - Check if Qdrant server is running - Verify port configuration - Check firewall settings</p> <p>Milvus Connection Issues - Ensure Milvus server is accessible - Verify API key for cloud deployments - Check collection permissions</p> <p>PGVector Extension Errors - Ensure PGVector extension is installed - Verify PostgreSQL version compatibility - Check database permissions</p> <p>Memory Issues - Reduce batch sizes for large datasets - Use disk-based storage instead of in-memory - Consider using a more scalable vector database</p>"},{"location":"user/installation/troubleshooting/#node-store-issues-node-store-issues","title":"Node Store Issues {#node-store-issues}","text":""},{"location":"user/installation/troubleshooting/#common-node-store-problems","title":"Common Node Store Problems","text":"<p>File Permission Errors - Check write permissions for <code>local_data/</code> directory - Ensure the application has access to create and modify files</p> <p>Data Corruption - Check for disk space issues - Verify file integrity of JSON files - Restore from backup if necessary</p> <p>PostgreSQL Connection Errors - Verify PostgreSQL service is running - Check host, port, and credentials - Ensure database and schema exist</p> <p>Performance Issues - Monitor memory usage for large datasets - Consider migrating to PostgreSQL for better performance</p>"},{"location":"user/installation/troubleshooting/#reranker-issues-reranker-issues","title":"Reranker Issues {#reranker-issues}","text":""},{"location":"user/installation/troubleshooting/#common-reranker-problems","title":"Common Reranker Problems","text":"<p>Memory Errors - Reduce <code>similarity_top_k</code> or <code>batch_size</code> - Use a smaller reranking model - Close other applications</p> <p>Slow Performance - Increase <code>batch_size</code> for faster processing - Use a smaller model - Reduce <code>similarity_top_k</code></p> <p>Poor Quality Results - Increase <code>similarity_top_k</code> for more candidates - Use a larger, more accurate model - Adjust <code>top_n</code> based on your needs</p>"},{"location":"user/usage/ingestion-reset/","title":"Reset Local Documents Database","text":"<p>When running in a local setup, you can remove all ingested documents by simply deleting all contents of <code>local_data</code> folder (except .gitignore).</p> <p>To simplify this process, you can use the command:</p> <pre><code>make wipe\n</code></pre> <p>Data Loss Warning</p> <p>This command will permanently delete all ingested documents and their associated metadata. Make sure you have backups if needed before running this command.</p>"},{"location":"user/usage/ingestion-reset/#advanced-usage","title":"Advanced Usage","text":"<p>You can actually delete your documents from your storage by using the API endpoint <code>DELETE</code> in the Ingestion API.</p>"},{"location":"user/usage/ingestion-reset/#delete-all-documents","title":"Delete All Documents","text":"<pre><code>curl -X DELETE \"http://localhost:8001/v1/ingest\"\n</code></pre> <p>Note: If authentication is enabled in your configuration (<code>server.auth.enabled: true</code>), add: <code>-H \"Authorization: Basic &lt;base64-encoded-credentials&gt;\"</code></p>"},{"location":"user/usage/ingestion-reset/#delete-specific-document","title":"Delete Specific Document","text":"<pre><code>curl -X DELETE \"http://localhost:8001/v1/ingest/{doc_id}\"\n</code></pre> <p>Replace <code>{doc_id}</code> with the actual document ID from the list endpoint.</p>"},{"location":"user/usage/ingestion-reset/#delete-documents-by-filename","title":"Delete Documents by Filename","text":"<p>You can delete specific documents by their filenames:</p> <pre><code>curl -X POST \"http://localhost:8001/v1/ingest/delete_by_filenames\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"filenames\": [\"document.pdf\", \"report.docx\"]\n  }'\n</code></pre>"},{"location":"user/usage/ingestion-reset/#manual-reset","title":"Manual Reset","text":"<p>If you prefer to manually reset the database:</p> <ol> <li>Stop the application if it's running</li> <li>Navigate to the data directory:    <pre><code>cd local_data/internal_assistant\n</code></pre></li> <li>Delete the vector database:    <pre><code>rm -rf qdrant/\n</code></pre></li> </ol> <p>Optionally, also remove:    <pre><code>rm -rf mitre_attack/    # MITRE ATT&amp;CK cached data\n</code></pre> 4. Restart the application</p> <p>Preserve Configuration</p> <p>The <code>make wipe</code> command preserves your configuration files while removing only the ingested document data.</p>"},{"location":"user/usage/ingestion-reset/#verification","title":"Verification","text":"<p>After resetting, you can verify that all documents have been removed:</p> <ol> <li>Check the UI: Navigate to http://localhost:8001 and verify no documents are listed</li> <li>Check the API: Use the status endpoint to confirm empty document count</li> <li>Check the logs: Look for confirmation messages in the application logs</li> </ol>"},{"location":"user/usage/ingestion-reset/#recovery","title":"Recovery","text":"<p>If you accidentally deleted documents and need to recover:</p> <ol> <li>Check for backups: Look for any backup files in your system</li> <li>Re-ingest documents: If you have the original files, you can re-ingest them</li> <li>Check version control: If documents were committed to git, you can restore them</li> </ol> <p>Prevention</p> <p>Consider setting up regular backups of your <code>local_data</code> folder to prevent accidental data loss.</p>"},{"location":"user/usage/ingestion/","title":"Document Ingestion","text":"<p>Ingest and manage documents in Internal Assistant.</p>"},{"location":"user/usage/ingestion/#ingestion-methods","title":"Ingestion Methods","text":""},{"location":"user/usage/ingestion/#1-web-ui-upload","title":"1. Web UI Upload","text":"<ol> <li>Navigate to http://localhost:8001</li> <li>Go to \"Documents\" tab</li> <li>Click \"Upload Documents\"</li> <li>Select files and upload</li> </ol>"},{"location":"user/usage/ingestion/#2-api-upload","title":"2. API Upload","text":"<p>Single file: <pre><code>curl -X POST \"http://localhost:8001/v1/ingest/file\" \\\n  -F \"file=@document.pdf\"\n</code></pre></p> <p>Multiple files: <pre><code>curl -X POST \"http://localhost:8001/v1/ingest/files\" \\\n  -F \"files=@file1.pdf\" \\\n  -F \"files=@file2.docx\"\n</code></pre></p>"},{"location":"user/usage/ingestion/#3-bulk-local-ingestion","title":"3. Bulk Local Ingestion","text":"<p>For local setups, ingest entire folders:</p> <pre><code>poetry run make ingest /path/to/folder\n</code></pre> <p>With file watching: <pre><code>poetry run make ingest /path/to/folder -- --watch\n</code></pre></p> <p>Important: Enable in settings first: <pre><code>data:\n  local_ingestion:\n    enabled: true\n    allow_ingest_from: [\"/path/to/folder\"]\n</code></pre></p>"},{"location":"user/usage/ingestion/#supported-file-types","title":"Supported File Types","text":"<ul> <li>Documents: PDF, DOCX, HWDOC, EPUB</li> <li>Text: TXT, MD</li> <li>Presentations: PPTX, PPT, PPTM</li> <li>Data: CSV, JSON</li> <li>Code: IPYNB</li> <li>Media: JPG, PNG, JPEG, MP3, MP4</li> <li>Email: MBOX</li> </ul>"},{"location":"user/usage/ingestion/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user/usage/ingestion/#ingestion-modes","title":"Ingestion Modes","text":"<p>Configure in <code>config/settings.yaml</code>:</p> <pre><code>embedding:\n  ingest_mode: parallel    # Options: simple, batch, parallel, pipeline\n  count_workers: 4         # Number of parallel workers\n</code></pre> <p>Mode comparison: - <code>simple</code>: Sequential, slowest but most stable - <code>batch</code>: Batch processing, moderate speed - <code>parallel</code>: Parallel processing, fastest for local - <code>pipeline</code>: Alternative to parallel</p>"},{"location":"user/usage/ingestion/#memory-management","title":"Memory Management","text":"<p>If running out of memory during ingestion:</p> <p>Use mock LLM mode (disables LLM during ingestion):</p> <pre><code>PGPT_PROFILES=mock poetry run make ingest /path/to/folder\n</code></pre> <p>Configuration: <pre><code>llm:\n  mode: mock\nembedding:\n  mode: local    # Keep embeddings active\n</code></pre></p>"},{"location":"user/usage/ingestion/#document-management","title":"Document Management","text":""},{"location":"user/usage/ingestion/#list-documents","title":"List Documents","text":"<pre><code>curl -X GET \"http://localhost:8001/v1/ingest/list\"\n</code></pre>"},{"location":"user/usage/ingestion/#delete-specific-document","title":"Delete Specific Document","text":"<pre><code>curl -X DELETE \"http://localhost:8001/v1/ingest/{doc_id}\"\n</code></pre>"},{"location":"user/usage/ingestion/#delete-all-documents","title":"Delete All Documents","text":"<pre><code>curl -X DELETE \"http://localhost:8001/v1/ingest\"\n</code></pre> <p>Or use the command: <pre><code>poetry run make wipe\n</code></pre></p>"},{"location":"user/usage/ingestion/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user/usage/ingestion/#memory-errors","title":"Memory Errors","text":"<ul> <li>Use <code>PGPT_PROFILES=mock</code> during ingestion</li> <li>Reduce <code>count_workers</code> in settings</li> <li>Process documents in smaller batches</li> </ul>"},{"location":"user/usage/ingestion/#slow-ingestion","title":"Slow Ingestion","text":"<ul> <li>Increase <code>count_workers</code> (if memory allows)</li> <li>Use <code>parallel</code> or <code>batch</code> mode</li> <li>Ensure using SSD storage</li> </ul>"},{"location":"user/usage/ingestion/#file-not-found","title":"File Not Found","text":"<ul> <li>Check file paths and permissions</li> <li>Verify file format is supported</li> <li>Ensure <code>local_ingestion.enabled: true</code> in settings</li> </ul>"},{"location":"user/usage/ingestion/#text-extraction-failures","title":"Text Extraction Failures","text":"<ul> <li>Check file corruption</li> <li>Try alternative file format</li> <li>Review ingestion logs in <code>local_data/logs/</code></li> </ul>"},{"location":"user/usage/ingestion/#best-practices","title":"Best Practices","text":"<ul> <li>Organize files by topic before ingesting</li> <li>Remove duplicates to avoid redundancy</li> <li>Use batch processing for large document sets</li> <li>Monitor resources during ingestion</li> <li>Validate results after ingestion completes</li> </ul>"},{"location":"user/usage/ingestion/#next-steps","title":"Next Steps","text":"<ul> <li>Reset Documents - Clear all ingested documents</li> <li>Summarization - Summarize ingested documents</li> <li>Configuration - Adjust ingestion settings</li> </ul>"},{"location":"user/usage/quickstart/","title":"Quick Start Guide","text":"<p>Get started with Internal Assistant quickly.</p>"},{"location":"user/usage/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11.9 installed</li> <li>Poetry 2.0+ installed</li> <li>Git installed</li> </ul> <p>See the Installation Guide for detailed setup instructions.</p>"},{"location":"user/usage/quickstart/#quick-install","title":"Quick Install","text":"<p>1. Clone and Install</p> <pre><code>git clone https://github.com/SecureYourGear/internal-assistant\ncd internal-assistant\npoetry install --extras \"ui llms-ollama embeddings-huggingface vector-stores-qdrant\"\n</code></pre> <p>2. Install Ollama</p> <p>Visit ollama.ai and install Ollama, then start the service:</p> <pre><code>ollama serve\n</code></pre> <p>3. Pull the Model</p> <pre><code>ollama pull foundation-sec-q4km:latest\n</code></pre> <p>4. Run Internal Assistant</p> <pre><code>poetry run make run\n</code></pre> <p>Access the UI at http://localhost:8001</p>"},{"location":"user/usage/quickstart/#first-steps","title":"First Steps","text":""},{"location":"user/usage/quickstart/#1-upload-documents","title":"1. Upload Documents","text":"<p>Via Web UI: 1. Navigate to http://localhost:8001 2. Go to the \"Documents\" tab 3. Click \"Upload Documents\" 4. Select your files (PDF, TXT, DOCX, etc.) 5. Click \"Upload\" and wait for processing</p> <p>Via API: <pre><code>curl -X POST \"http://localhost:8001/v1/ingest/file\" \\\n  -F \"file=@your-document.pdf\"\n</code></pre></p>"},{"location":"user/usage/quickstart/#2-chat-with-your-documents","title":"2. Chat with Your Documents","text":"<p>Via Web UI: 1. Go to the \"Chat\" tab 2. Select \"Query documents\" mode 3. Ask questions about your uploaded documents 4. Get AI-powered responses with source citations</p> <p>Via API: <pre><code>curl -X POST \"http://localhost:8001/v1/chat/completions\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What are the main security findings in the documents?\"}\n    ],\n    \"use_context\": true\n  }'\n</code></pre></p>"},{"location":"user/usage/quickstart/#3-threat-intelligence-feeds","title":"3. Threat Intelligence Feeds","text":"<ol> <li>Navigate to the \"Feeds\" tab</li> <li>Browse available security RSS feeds</li> <li>Click \"Add Feed\" to subscribe to threat intelligence sources</li> <li>View latest security updates and alerts</li> </ol>"},{"location":"user/usage/quickstart/#configuration-profiles","title":"Configuration Profiles","text":"<p>Run Internal Assistant with different configurations:</p> <p>Default (Ollama): <pre><code>poetry run make run\n</code></pre></p> <p>Mock Mode (Testing): <pre><code>PGPT_PROFILES=mock poetry run make run\n</code></pre></p> <p>Development Mode: <pre><code>poetry run make dev\n</code></pre></p>"},{"location":"user/usage/quickstart/#common-tasks","title":"Common Tasks","text":""},{"location":"user/usage/quickstart/#clear-all-documents","title":"Clear All Documents","text":"<pre><code>poetry run make wipe\n</code></pre>"},{"location":"user/usage/quickstart/#view-logs","title":"View Logs","text":"<pre><code>tail -f local_data/logs/SessionLog*.log\n</code></pre>"},{"location":"user/usage/quickstart/#run-tests","title":"Run Tests","text":"<pre><code>poetry run make test\n</code></pre>"},{"location":"user/usage/quickstart/#check-compatibility","title":"Check Compatibility","text":"<pre><code>poetry run make compatibility-check\n</code></pre>"},{"location":"user/usage/quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Service won't start: - Ensure Ollama is running: <code>ollama list</code> - Check port 8001 is available - Review logs in <code>local_data/logs/</code></p> <p>Document upload fails: - Check file format is supported - Ensure sufficient disk space - Review ingestion logs</p> <p>Model not found: - Pull the model: <code>ollama pull foundation-sec-q4km:latest</code> - Verify Ollama is running: <code>ollama list</code></p> <p>For more issues, see the Troubleshooting Guide.</p>"},{"location":"user/usage/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Document Management: Learn more about Document Ingestion</li> <li>Configuration: Customize your Settings</li> <li>LLM Options: Explore different LLM Providers</li> <li>API Reference: Use the API Documentation</li> </ul>"},{"location":"user/usage/summarize/","title":"Document Summarization","text":"<p>Generate concise summaries from ingested documents or text.</p>"},{"location":"user/usage/summarize/#overview","title":"Overview","text":"<p>The Summarize feature extracts key information from documents, saving time and improving comprehension of large content volumes.</p> <p>Use Cases: - Research papers - Extract findings and conclusions - Business reports - Get executive summaries - Legal documents - Identify key clauses - Technical documentation - Understand main concepts - Security reports - Highlight critical threats</p>"},{"location":"user/usage/summarize/#usage","title":"Usage","text":""},{"location":"user/usage/summarize/#web-ui","title":"Web UI","text":"<ol> <li>Navigate to http://localhost:8001</li> <li>Click \"Summarize\" tab</li> <li>Choose input source:</li> <li>Direct Text: Paste content directly</li> <li>Ingested Document: Select from document library</li> <li>Configure options:</li> <li>Summary length (short, medium, long)</li> <li>Custom instructions</li> <li>Summary style</li> <li>Click \"Generate Summary\"</li> <li>Copy or download results</li> </ol>"},{"location":"user/usage/summarize/#api","title":"API","text":"<p>Basic Summarization: <pre><code>curl -X POST \"http://localhost:8001/v1/summarize\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"text\": \"Your document text here...\",\n    \"max_length\": 200\n  }'\n</code></pre></p> <p>With Custom Instructions: <pre><code>curl -X POST \"http://localhost:8001/v1/summarize\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"text\": \"Your document text here...\",\n    \"max_length\": 300,\n    \"instructions\": \"Focus on technical details and key findings\",\n    \"style\": \"technical\"\n  }'\n</code></pre></p> <p>Streaming Mode: <pre><code>curl -X POST \"http://localhost:8001/v1/summarize/stream\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"text\": \"Your document text here...\",\n    \"max_length\": 200\n  }'\n</code></pre></p>"},{"location":"user/usage/summarize/#configuration-options","title":"Configuration Options","text":"Parameter Type Description Default <code>text</code> string Text to summarize Required <code>max_length</code> integer Maximum summary length (words) 200 <code>instructions</code> string Custom instructions - <code>style</code> string Summary style (technical, executive, academic) \"general\""},{"location":"user/usage/summarize/#summary-styles","title":"Summary Styles","text":"<p>Technical: <pre><code>{\n  \"style\": \"technical\",\n  \"instructions\": \"Focus on technical details, methodologies, and quantitative results\"\n}\n</code></pre></p> <p>Executive: <pre><code>{\n  \"style\": \"executive\",\n  \"instructions\": \"High-level overview with key business implications\"\n}\n</code></pre></p> <p>Academic: <pre><code>{\n  \"style\": \"academic\",\n  \"instructions\": \"Emphasize research methodology, findings, and conclusions\"\n}\n</code></pre></p> <p>Security-Focused (for threat reports): <pre><code>{\n  \"style\": \"security\",\n  \"instructions\": \"Highlight threats, vulnerabilities, indicators of compromise, and recommended actions\"\n}\n</code></pre></p>"},{"location":"user/usage/summarize/#examples","title":"Examples","text":""},{"location":"user/usage/summarize/#security-report-summary","title":"Security Report Summary","text":"<p>Input: CVE vulnerability report</p> <p>Configuration: <pre><code>{\n  \"max_length\": 250,\n  \"style\": \"security\",\n  \"instructions\": \"Focus on vulnerability severity, affected systems, and mitigation steps\"\n}\n</code></pre></p> <p>Output: \"CVE-2024-1234 is a critical remote code execution vulnerability in Apache Framework 2.x affecting versions 2.0-2.8. CVSS score 9.8. Attackers can exploit via crafted HTTP requests to gain unauthorized system access. Affects approximately 50,000 internet-exposed servers. Immediate patching to version 2.9 required. No workarounds available.\"</p>"},{"location":"user/usage/summarize/#best-practices","title":"Best Practices","text":""},{"location":"user/usage/summarize/#content-preparation","title":"Content Preparation","text":"<ul> <li>Remove formatting artifacts</li> <li>Ensure logical text flow</li> <li>Consider breaking very long documents into sections</li> </ul>"},{"location":"user/usage/summarize/#configuration","title":"Configuration","text":"<ul> <li>Match summary length to use case</li> <li>Be specific in instructions</li> <li>Choose appropriate style for audience</li> </ul>"},{"location":"user/usage/summarize/#quality-control","title":"Quality Control","text":"<ul> <li>Always review generated summaries</li> <li>Cross-check with original content</li> <li>Iterate on parameters for better results</li> </ul>"},{"location":"user/usage/summarize/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user/usage/summarize/#poor-summary-quality","title":"Poor Summary Quality","text":"<ul> <li>Increase <code>max_length</code> for more detail</li> <li>Provide more specific <code>instructions</code></li> <li>Check input text quality</li> </ul>"},{"location":"user/usage/summarize/#slow-performance","title":"Slow Performance","text":"<ul> <li>Reduce <code>max_length</code></li> <li>Use streaming mode for large documents</li> <li>Break documents into smaller sections</li> </ul>"},{"location":"user/usage/summarize/#memory-issues","title":"Memory Issues","text":"<ul> <li>Process documents in smaller batches</li> <li>Use streaming mode</li> <li>Monitor system resources</li> </ul>"},{"location":"user/usage/summarize/#integration","title":"Integration","text":"<p>With Document Ingestion: - Automatically summarize documents after ingestion - Create summary metadata for search - Index summaries for quick retrieval</p> <p>With Chat: - Use summaries as context for responses - Generate on-demand summaries during conversations - Include summary references in chat responses</p>"},{"location":"user/usage/summarize/#next-steps","title":"Next Steps","text":"<ul> <li>Document Ingestion - Upload documents to summarize</li> <li>Configuration - Adjust summarization settings</li> <li>API Reference - Full API documentation</li> </ul>"}]}